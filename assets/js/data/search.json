[ { "title": "Output", "url": "/posts/output/", "categories": "", "tags": "", "date": "2022-09-27 00:00:00 +0700", "snippet": "from google.colab import drivedrive.mount('/content/drive')Giới Thiệu Ngôn Ngữ PythonGiới thiệuCài đặt Python trên Windows, Linux và MacLinuxWindowsMacChọn IDE và chương trình Python đầu tiênChọn IDEChương trình Python đầu tiênPython Cơ BảnCác kiểu dữ liệu PythonPython numbersPython StringsPython Booleanspython containsListsTuplesSetsDictionariesArrayCác loại biến trong PythonBiểu thức điều kiệnVòng lặpVòng lặp forVòng lặp whileKỹ thuật vòng lặpHàmHàm cơ bảnHàm vô danh (Lambda)Module và Pakage trong PythonClass và Object trong PythonPython Nâng caoLàm việc với filePython cũng hỗ trợ xử lý tệp và cho phép người dùng xử lý tệp, tức là đọc và ghitệp, cùng với nhiều tùy chọn xử lý tệp khác, hoạt động trên tệp. Khái niệm xử lýtệp đã trải dài trên nhiều ngôn ngữ khác, nhưng việc triển khai phức tạp hoặcdài dòng, nhưng giống như các khái niệm khác của Python, khái niệm này ở đâycũng dễ dàng và ngắn gọn. Python xử lý các tệp khác nhau dưới dạng văn bản hoặcnhị phân và điều này rất quan trọng. Mỗi dòng mã bao gồm một chuỗi các ký tự vàchúng tạo thành một tệp văn bản.Mở File trong Pythonf = open(filename, mode)Ví dụ:file = open('/content/drive/MyDrive/Python3-Tutorial/Code/Python/files/test.txt', 'r')Đọc File trong Pythonfile.read ()Để đọc một file ta cần mở file bằng cú pháp để đọc, sử dụng mode read ‘r’.file = open('/content/drive/MyDrive/Python3-Tutorial/Code/Python/files/test.txt', 'r')file.read ()Ghi File trong Pythonf = write(filename, mode)Để ghi một file ta cần mở file bằng cú pháp để ghi, sử dụng mode write ‘w’,append ‘a’ hoặc mode tạo độc quyền ‘x’Bạn cần cẩn thận với chế độ ‘w’, vì nó ghi đè lên nội dung nếu file đã tồn tại,các dữ liệu trước đó sẽ bị xóa.Nếu bạn ghi vào file dạng nhị phân các chuỗi văn bản hoặc chuỗi dạng byte thìkết quả trả về sẽ là số kí tự được ghi vào file.file = open('/content/drive/MyDrive/Python3-Tutorial/Code/Python/files/test.txt', 'w')file.write(\"toi la lap trinh vien\")file.close()Một số phương thức khácIterator, Generator, Closure, Decorator, Regex và @property trong pythonIteratorIterator là các đối tượng cho phép ta lấy từng phần tử của nó, hành động này cóthể được lặp đi lặp lại.Trình lặp là một đối tượng có thể được lặp lại, có nghĩalà bạn có thể duyệt qua tất cả các giá trị.Về mặt kỹ thuật, trong Python, một trình vòng lặp là một đối tượng thực hiệngiao thức trình vòng lặp, bao gồm các phương thức iter() và next().# Thí dụmytuple = (\"apple\", \"banana\", \"cherry\")myit = iter(mytuple)print(next(myit))print(next(myit))print(next(myit))GeneratorClosureDecoratorRegex@propertyMa trận trong PythonĐối tượng Iterator trong PythonGenerator trong PythonClosure trong PythonDecorator trong Python@property trong PythonRegEx trong PythonNumpy, pandas và matplotlibNumpyPandasMatplotlibPython GUIPyySimpleGUIPython ToolsPython WebPython MobileappPython GamePython Machine - AI!pip install https://github.com/aaren/notedown/tarball/master!notedown /content/drive/MyDrive/Python3-Tutorial/Code/Python/all.ipynb --to test.md!notedown /content/drive/MyDrive/Python3-Tutorial/Code/Python/all.ipynb --to markdown --strip &gt; output.md" }, { "title": "Tổng quan về xử lý ngôn ngữ tự nhiên", "url": "/posts/T%E1%BB%95ng-quan-v%E1%BB%81-NLP/", "categories": "Python, Machine-learning", "tags": "ML, AI", "date": "2022-09-26 00:00:00 +0700", "snippet": " I. Giới thiệu về Xử lý ngôn ngữ tự nhiên (Natural Language Proccessing)Xử lý ngôn ngữ tự nhiên cung cấp một bộ công cụ và thuật toán rất cần thiết để hiểu và xử lý khối lượng lớn dữ liệu phi cấu trúc trong thế giới hiện nay. Hiện nay Deep learning đã được áp dụng rộng rãi cho nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên vì hiệu suất deep learning hiệu quả đáng kể trong các nhiệm vụ như phân loại hình ảnh, nhận dạng giọng nói và tạo văn bản thực tế,… Tensorflow là một deep learning framework trực quan và hiệu quả nhất cho các nhiệm vụ này. 1.Xử lý ngôn ngữ tự nhiên là gìMục tiêu của xử lý ngôn ngữ tự nhiên là làm cho máy móc hiểu các ngôn ngữ nói và viết của chúng ta. Xử lý ngôn ngữ tự nhiên có mặt ở khắp nơi và đã là một phần lớn trong cuộc sống của con người. Trợ lý ảo (Virtual Assistants) chẳng hạn như Google Assistant, Cortana, Alexa và Apple Siri,.. phần lớn là các hệ thống NLP.NLP là một lĩnh vực nghiên cứu cực kỳ thách thức vì các từ và ngữ nghĩa có mối quan hệ phi tuyến tính rất phức tạp. Vấn đề còn khó khăn hơn khi mỗi ngôn ngữ có ngữ pháp, cú pháp và từ vựng riêng. Do đó, xử lý dữ liệu văn bản liên quan đến các nhiệm vụ phức tạo khác nhau như phân tích cú pháp văn bản, hiểu cấu trúc ngữ pháp cơ bản của ngôn ngữ. Ví dụ trong hai câu này,”tôi đi trên đường” và “tôi thêm đường vào nước cam”, từ “đường” có hai ý nghĩa hoàn toàn khác nhau, do bối cảnh mà nó được sử dụng.Để hiểu bối cảnh mà từ đang được sử dụng. Học máy đã trở thành một yếu tố chính cho NLP, giúp hoàn thành các nhiệm vụ đã nói ở trên thông qua học máy. 2. Nhiệm vụ của xử lý ngôn ngữ tự nhiên Tokenization: là nhiệm vụ tách một kho văn bản thành các đơn vị nhỏ hơn (ví dụ: từ hoặc ký tự). Mặc dù nó có vẻ bình thường đối với một số ngôn ngữ như tiếng Anh nhưng lại khó khăn đối với các ngôn ngữ như tiếng Nhật vì các từ không được phân định bởi khoảng cách hay dấu chấm câu Word-Sense Disambiguation: là nhiệm vụ xác định đúng ý nghĩa của từ Nhận dạng thực thể (Named Entity Recognition(NER)): cố gắng trích xuất các thực thể (ví dụ: người, vị trí, tổ chức) từ một phần nhất định của văn bản. Ví dụ, mẫu câu , “John gave Mary two apples at school on Monday” sẽ được chuyển đổi thành “[John]name gave [Mary]name [two]number apples at [school]organization on [Monday]time”. NER là một chủ đề bắt buộc trong các lĩnh vực như truy xuất thông tin. Gán nhãn từ loại (Part-of-Speech tagging POS): là nhiệm vụ gán các từ loại cho các phần của văn bản. Nó có thể là các thẻ danh từ, động từ, tính từ, trạng từ và giới từ,… Phân loại câu/tóm tắt văn bản: Phân loại câu hoặc bản tóm tắt (ví dụ, đánh giá phim) có nhiều trường hợp sử dụng như phát hiện thư rác, phân loại bài báo (ví dụ: chính trị, công nghệ và thể thao) và xếp hạng đánh giá sản phẩm (nghĩa là tích cực hoặc tiêu cực). Điều này đạt được bằng cách đào tạo một mô hình phân loại với dữ liệu được dán nhãn (nghĩa là đánh giá được chú thích bởi con người, với nhãn tích cực hoặc tiêu cực). Tạo văn bản: Trong tạo văn bản, một mô hình học tập (ví dụ: mạng thần kinh) được đào tạo bằng văn bản (một bộ sưu tập lớn các tài liệu văn bản) và sau đó nó dự đoán văn bản mới.Ví dụ, mô hình ngôn ngữ có thể xuất hiện một câu chuyện khoa học viễn tưởng hoàn toàn mới bằng cách sử dụng các câu chuyện khoa học viễn tưởng hiện có để đào tạo. Gần đây, Openai đã phát hành một mô hình ngôn ngữ được gọi là Openai-GPT-2, có thể tạo ra văn bản cực kỳ thực tế Trả lời câu hỏi (Question Answering): Kỹ thuật QA có giá trị thương mại cao và các kỹ thuật như vậy được tìm thấy tại nền tảng của Chatbots và Virtual Assistant (ví dụ: Google Assistant và Apple Siri) Dịch máy (Machine Translation): MT là nhiệm vụ chuyển đổi câu/cụm từ từ ngôn ngữ nguồn (ví dụ: tiếng Đức) sang ngôn ngữ đích (ví dụ: tiếng Anh). Đây là một nhiệm vụ rất khó khăn, vì các ngôn ngữ khác nhau có các cấu trúc cú pháp khác nhau, điều đó có nghĩa là nó không phải là một chuyển đổi một-một. Hơn nữa, các mối quan hệ từ ngữ giữa các ngôn ngữ có thể là một-nhiều, một-một, nhiều-một hoặc nhiều-nhiều.Trong hình dưới, ta có thể thấy phân loại phân cấp của các nhiệm vụ NLP khác nhau được phân loại thành nhiều loại khác nhau. Đó là một nhiệm vụ khó khăn để gán một nhiệm vụ NLP cho một phân loại duy nhất. Chúng ta sẽ chia các danh mục thành hai loại chính: dựa trên ngôn ngữ (màu sáng với văn bản đen) và dựa trên công thức vấn đề (màu tối với văn bản trắng). Vấn đề ngôn ngữ có hai loại: cú pháp (dựa trên cấu trúc) và ngữ nghĩa (dựa trên ý nghĩa). Vấn đề dựa trên công thức có vấn đề có ba loại: các nhiệm vụ tiền xử lý (các tác vụ được thực hiện trên dữ liệu văn bản trước khi cung cấp cho một mô hình), nhiệm vụ phân loại(nhiệm vụ mà chúng ta cố gắng gán văn bản đầu vào cho một hoặc nhiều danh mục từ một tập hợp các danh mục được xác định trước) và các nhiệm vụ tạo ra (các nhiệm vụ mà chúng tôi cố gắng tạo ra một đầu ra văn bản mới) 3. Cách tiếp cận truyền thống để xử lý ngôn ngữ tự nhiênCách tiếp cận truyền thống hoặc cổ điển để giải quyết NLP là một luồng tuần tự của một số bước chính và đó là một cách tiếp cận thống kê. Khi chúng ta xem xét kỹ hơn về mô hình học tập NLP truyền thống, chúng ta sẽ có thể thấy một tập hợp các tác vụ riêng biệt đang diễn ra, chẳng hạn như tiền xử lý dữ liệu bằng cách xóa dữ liệu không mong muốn,kỹ thuật tính năng (feature engineering) để có được các biểu diễn tốt về dữ liệu văn bản, tìm hiểu để sử dụng các thuật toán học máy với sự trợ giúp của dữ liệu đào tạo và dự đoán đầu ra cho dữ liệu. Trong đó, kỹ thuật tính năng là bước tốn thời gian và quan trọng nhất để đạt được hiệu suất tốt trên một nhiệm vụ NLP nhất định. 3.Hiểu cách tiếp cận truyền thốngĐầu tiên,văn bản cần được xử lý trước, tập trung vào việc giảm từ vựng và phân tâm.Tiếp đến đến là một số bước kỹ thuật tính năng (feature engineering). Mục tiêu chính của kỹ thuật tính năng là làm cho việc học dễ dàng hơn cho các thuật toán. Thông thường các tính năng được thiết kế bằng tay và thiên vị đối với sự hiểu biết của con người về một ngôn ngữ. Kỹ thuật tính năng là vô cùng quan trọng đối với các thuật toán NLP cổ điển, và do đó, các hệ thống hiệu suất tốt nhất thường có các tính năng tốt nhất. Ví dụ: đối với một nhiệm vụ phân loại tình cảm, bạn có thể biểu thị một câu có cây phân tích và gán các nhãn dương, âm hoặc trung tính cho mỗi nút/cây con trong cây để phân loại câu đó là dương hoặc âm. Ngoài ra, giai đoạn kỹ thuật tính năng có thể sử dụng các tài nguyên bên ngoài như WordNet (cơ sở dữ liệu từ vựng có thể cung cấp hiểu biết về cách các từ khác nhau có liên quan với nhau - ví dụ: từ đồng nghĩa) để phát triển các tính năng tốt hơn. Chúng ta sẽ sớm xem xét một kỹ thuật kỹ thuật tính năng đơn giản được gọi là bag-of-words.Tiếp theo, thuật toán học tập học cách thực hiện tốt trong nhiệm vụ đã cho bằng cách sử dụng các tính năng thu được và tùy chọn các tài nguyên bên ngoài. Ví dụ, đối với một nhiệm vụ tóm tắt văn bản, một kho văn bản song song chứa các cụm từ phổ biến và các chú giải cô đọng sẽ là một nguồn tài nguyên bên ngoài tốt. Cuối cùng, dự đoán xảy ra. Dự đoán rất đơn giản, trong đó bạn sẽ cung cấp một đầu vào mới và chứa các dự đoán bằng cách chuyển tiếp đầu vào thông qua mô hình học tập. Toàn bộ quá trình của phương pháp truyền thống được mô tả bên dưới 4.Hạn chế của phương pháp truyền thốngCác bước tiền xử lý được sử dụng trong NLP truyền thống buộc đánh đổi thông tin có khả năng hữu ích được nhúng trong văn bản (ví dụ: dấu câu) để làm cho việc học khả thi bằng cách giảm từ vựng.Kỹ thuật tính năng mất rất nhiều thời gian. Để thiết kế một hệ thống đáng tin cậy, các tính năng tốt cần phải được nghĩ ra. Quá trình này có thể rất tẻ nhạt vì các không gian tính năng khác nhau cần được khám phá và đánh giá rộng rãi. Ngoài ra, để có hiệu quả các tính năng mạnh mẽ, cần có chuyên môn về miền, có thể khan hiếm và tốn kém cho một số nhiệm vụ NLP nhất định.Các tài nguyên bên ngoài khác nhau là cần thiết để nó hoạt động tốt, và không có nhiều tài nguyên miễn phí. Các tài nguyên bên ngoài như vậy thường bao gồm thông tin được tạo thủ công được lưu trữ trong cơ sở dữ liệu lớn. Tạo một cho một nhiệm vụ cụ thể có thể mất vài năm, phụ thuộc vào mức độ nghiêm trọng của nhiệm vụ. 5.Phương pháp học sâu để xử lý ngôn ngữ tự nhiênCác mô hình sâu đã tạo ra một làn sóng thay đổi mô hình trong nhiều lĩnh vực trong học máy, vì các mô hình sâu đã học được các tính năng phong phú từ dữ liệu thô thay vì sử dụng các tính năng bị hạn chế do con người thiết kế. Điều này do đó khiến kỹ thuật tính năng gây phiền nhiễu và đắt tiền bị lỗi thời. Với điều này, các mô hình sâu đã làm cho quy trình làm việc truyền thống hiệu quả hơn, vì các mô hình sâu thực hiện học tập tính năng và học tập nhiệm vụ một cách đồng thời. Hơn nữa, do số lượng lớn các tham số (nghĩa là trọng số) trong một mô hình sâu, nó có thể bao gồm nhiều tính năng hơn đáng kể so với con người có thể thiết kế. Tuy nhiên, các mô hình sâu được coi là một hộp đen do khả năng diễn giải kém của mô hình. Một mạng lưới thần kinh sâu về cơ bản là một mạng lưới thần kinh nhân tạo có lớp đầu vào, nhiều lớp ẩn được kết nối với nhau ở giữa, và cuối cùng, một lớp đầu ra (ví dụ: phân loại hoặc hồi quy). Như bạn có thể thấy, điều này tạo thành một mô hình từ đầu đến cuối từ dữ liệu thô đến dự đoán. Các lớp ẩn này ở giữa cung cấp sức mạnh cho các mô hình sâu vì chúng chịu trách nhiệm học các tính năng tốt từ dữ liệu thô. 6.Hiểu một mô hình sâu đơn giản - một mạng lưới thần kinh được kết nối đầy đủBây giờ, hãy để có một cái nhìn kỹ hơn về một mạng lưới thần kinh sâu để có được sự hiểu biết tốt hơn. Mặc dù có rất nhiều biến thể khác nhau của các mô hình sâu, hãy nhìn vào một trong những mô hình sớm nhất có từ năm 1950 mô tả một a fully connected neural network (FCNN) có ba lớp tiêu chuẩnMục tiêu của FCNN là ánh xạ đầu vào (ví dụ: hình ảnh hoặc câu) đến một nhãn hoặc chú thích nhất định (ví dụ: danh mục đối tượng cho hình ảnh). Điều này đạt được bằng cách sử dụng đầu vào X để tính toán H - biểu diễn ẩn của X - sử dụng một phép biến đổi như ℎ=𝜎(𝑊 * h + b); Ở đây, W là trọng số và b là độ lệch của FCNN, và 𝜎 là chức năng kích hoạt sigmoid. Mạng thần kinh sử dụng các chức năng kích hoạt phi tuyến tính ở mỗi lớp. Kích hoạt sigmoid là một trong những kích hoạt như vậy. Nó là một chuyển đổi phần tử được áp dụng cho đầu ra của một lớp, trong đó đầu ra sigmoidal của x được cho bởi, 𝜎(𝑥) = 1/(1+𝑒-x).Tiếp theo, một trình phân loại được đặt trên đầu FCNN cung cấp khả năng tận dụng các tính năng đã học trong các lớp ẩn để phân loại đầu vào. Trình phân loại là một phần của FCNN và một lớp ẩn khác với một số trọng số Ws và thiên vị Bs. Ngoài ra, chúng ta có thể tính toán đầu ra cuối cùng của FCNN là output =softmax(𝑊𝑠 ∗ ℎ+ B𝑠).Ví dụ, một phân loại SoftMax có thể được sử dụng cho các vấn đề phân loại đa nhãn. Nó cung cấp một biểu diễn chuẩn hóa của đầu ra điểm số của lớp phân loại. Đó là, nó sẽ tạo ra một phân phối xác suất hợp lệ trên các lớp trong lớp phân loại. Nhãn được coi là nút đầu ra có giá trị softmax cao nhất. Sau đó, với điều này, chúng ta có thể xác định tổn thất phân loại được tính là sự khác biệt giữa nhãn đầu ra dự đoán và nhãn đầu ra thực tế. Một ví dụ về chức năng mất mát như vậy là mất bình phương trung bình (mean squared loss).Tiếp theo, các tham số mạng thần kinh, W, B, WS và BS, được tối ưu hóa bằng trình tối ưu hóa ngẫu nhiên tiêu chuẩn (ví dụ: giảm độ dốc ngẫu nhiên) để giảm sự mất phân loại của tất cả các đầu vào. Hình dưới mô tả quá trình được giải thích trong đoạn này cho FCNN ba lớp.Có thể sử dụng mạng thần kinh (có thể sâu hoặc nông, tùy thuộc vào độ khó của nhiệm vụ) cho nhiệm vụ này bằng cách tuân thủ quy trình làm việc sau: Mã thông báo (Tokenize) câu thành các từ. Chuyển đổi các câu thành một biểu diễn số có kích thước cố định (ví dụ: biểu diễn túi của các từ). Một biểu diễn có kích thước cố định là cần thiết vì các mạng thần kinh được kết nối đầy đủ đòi hỏi một đầu vào có kích thước cố định. Cung cấp các đầu vào số vào mạng thần kinh, dự đoán đầu ra (dương hoặc âm) và so sánh với mục tiêu thực. Tối ưu hóa mạng lưới thần kinh bằng cách sử dụng chức năng tổn thất mong muốn. 7. Một số nền tảng tính toán dựa trên đám mây phổ biến• Google Colab: https://colab.research.google.com/• Google Cloud Platform (GCP): https://cloud.google.com/• Amazon Web Services (AWS): https://aws.amazon.com/ II.Hiểu về Tensorflow 2 1.Tensorflow là gì?Tensorflow là một nguồn mở, được phát hành bởi Google, chủ yếu nhằm giảm bớt các chi tiết đau đớn khi thực hiện mạng lưới thần kinh (ví dụ, tính toán các dẫn xuất (derivatives) của trọng lượng của mạng lưới thần kinh). TensorFlow tiến thêm một bước bằng cách cung cấp các triển khai hiệu quả các tính toán số đó bằng cách sử dụng kiến trúc thiết bị hợp nhất tính toán (CUDA), là một nền tảng tính toán song song được NVIDIA giới thiệu. 2.Bắt đầu với TensorFlow 2Bây giờ, hãy để tìm hiểu về một vài thành phần thiết yếu trong khung TensorFlow bằng cách làm việc thông qua một ví dụ về mã. Hãy để viết một ví dụ để thực hiện tính toán sau, điều này rất phổ biến đối với các mạng thần kinh:Tính toán này bao gồm những gì xảy ra trong một lớp duy nhất của một mạng nơ-ron được kết nối hoàn toàn. Ở đây W và x là ma trận và b là một vectơ. Sau đó,”.’ biểu thị dot. sigmoid là một phép biến đổi phi tuyến tính được đưa ra bởi phương trình sau:Đầu tiên, chúng ta sẽ cần nhập TensorFlow và Numpy. Numpy là một khung tính toán khoa học khác cung cấp các hoạt động toán học và các hoạt động khác để thao tác dữ liệu. Nhập vào chúng là điều cần thiết trước khi bạn chạy bất kỳ loại hoạt động liên quan đến TensorFlow hoặc Numpy trong Python:import tensorflow as tf import numpy as npĐầu tiên, chúng tôi sẽ viết một hàm có thể lấy các đầu vào X, W và B và thực hiện tính toán này cho chúng tôi:def layer(x, W, b): # Building the graph h = tf.nn.sigmoid(tf.matmul(x,W) + b) # Operation to perform return hTiếp theo, chúng tôi thêm một công cụ trang trí Python có tên TF.Function như sau@tf.functiondef layer(x, W, b): # Building the graph h = tf.nn.sigmoid(tf.matmul(x,W) + b) # Operation to perform return hNói một cách đơn giản, một decorator Python chỉ là một hàm khác. Một decorator Python cung cấp một cách sạch sẽ để gọi một hàm khác bất cứ khi nào bạn gọi hàm decorated. Nói cách khác, mỗi khi hàm layer() được gọi, tf.function() được gọi. Điều này có thể được sử dụng cho các mục đích khác nhau, chẳng hạn như:• Ghi nhật ký nội dung và hoạt động trong một hàm• Xác thực các đầu vào và đầu ra của hàm khácKhi hàm layer() đi qua tf.function (), TensorFlow sẽ theo dõi nội dung (nói cách khác, các hoạt động và dữ liệu) trong hàm và tự động xây dựng biểu đồ tính toán.Biểu đồ tính toán (còn được gọi là biểu đồ DataFlow) xây dựng DAG (biểu đồ acyclic có hướng) hiển thị loại đầu vào nào được yêu cầu và loại tính toán cần được thực hiện trong chương trìnhTrong ví dụ này, hàm layer() tạo ra H bằng cách sử dụng đầu vào X, W và B và một số phương tiện chuyển đổi hoặc các operations như + và tf.matmul ():Nếu chúng ta nhìn vào một sự tương tự cho một DAG, nếu bạn nghĩ về đầu ra như một chiếc bánh, thì biểu đồ sẽ là công thức để làm cho bánh đó sử dụng các thành phần (nghĩa là đầu vào).Tính năng xây dựng biểu đồ tính toán này tự động trong TensorFlow được gọi là AutoGraph. AutoGraph không chỉ nhìn vào các operations trong hàm được thông qua; Nó cũng xem xét kỹ lưỡng dòng chảy của các hoạt động. Điều này có nghĩa là bạn có thể có nếu các câu lệnh if, hoặc các vòng lặp for/while trong hàm của bạn và AutoGraph sẽ quan tâm chúng khi xây dựng biểu đồ.Tiếp theo, bạn có thể sử dụng chức năng này ngay lập tức, như saux = np.array([[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]],dtype=np.float32)Ở đây, x là một mảng numpy đơn giảninit_w = tf.initializers.RandomUniform(minval=-0.1, maxval=0.1)(shape=[10,5])W = tf.Variable(init_w, dtype=tf.float32, name='W') init_b = tf.initializers.RandomUniform()(shape=[5])b = tf.Variable(init_b, dtype=tf.float32, name='b')W và B là các biến TensorFlow được xác định bằng đối tượng TF.Varable. W và B là tensor. Một tensor về cơ bản là một mảng N chiều. Ví dụ, một vectơ một chiều hoặc ma trận hai chiều được gọi là tensor. tf.variable là một cấu trúc có thể thay đổi, có nghĩa là các giá trị trong tensor được lưu trữ trong biến đó có thể thay đổi theo thời gian. Ví dụ, các biến được sử dụng để lưu trữ các trọng số mạng thần kinh, thay đổi trong quá trình tối ưu hóa mô hình.Ngoài ra, lưu ý rằng với W và B, chúng tôi cung cấp một số đối số quan trọng, chẳng hạn như sau:init_w = tf.initializers.RandomUniform(minval=-0.1, maxval=0.1)(shape=[10,5])init_b = tf.initializers.RandomUniform()(shape=[5])Chúng được gọi là các bộ khởi tạo biến và là các tensor sẽ được gán cho các biến W và B ban đầu. Một biến phải có một giá trị ban đầu được cung cấp. Ở đây, tf.initializer.randomuniform có nghĩa là chúng ta thống nhất các giá trị mẫu giữa minval (-0.1) và maxval (0,1) để gán các giá trị cho các tensor. Có nhiều bộ khởi tạo khác nhau được cung cấp trong TensorFlow (https: // www.tensorflow.org/api_docs/python/tf/keras/initializer). Nó cũng rất quan trọng để xác định hình dạng của bộ khởi tạo của bạn khi bạn đang xác định chính bộ khởi tạo. Thuộc tính hình dạng xác định kích thước của từng chiều của tenxơ đầu ra. Ví dụ: nếu hình dạng là [10, 5], điều này có nghĩa là nó sẽ là cấu trúc hai chiều và sẽ có 10 phần tử trên trục 0 (hàng) và 5 phần tử trên trục 1 (cột):h = layer(x,W,b)Cuối cùng, H được gọi là tensorflow tensor nói chung. Một tensorflow tensor là một cấu trúc bất biến. Khi một giá trị được gán cho tensorflow tensor, nó không thể thay đổi.Như bạn có thể thấy, thuật ngữ tensor được sử dụng theo hai cách:• để chỉ một mảng n chiều• để tham khảo cấu trúc dữ liệu bất biến trong tensorflowCuối cùng, bạn có thể thấy ngay giá trị của Hprint(f\"h = {h.numpy()}\")@tf.functiondef layer(x, W, b): # Building the graph h = tf.nn.sigmoid(tf.matmul(x,W) + b) # Operation to be performed return hx = np.array([[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]], dtype=np.float32) # Variableinit_w = tf.initializers.RandomUniform(minval=-0.1, maxval=0.1)(shape=[10,5])W = tf.Variable(init_w, dtype=tf.float32, name='W') # Variableinit_b = tf.initializers.RandomUniform()(shape=[5])b = tf.Variable(init_b, dtype=tf.float32, name='b') h = layer(x,W,b)print(f\"h = {h.numpy()}\") 3.Đầu vào, biến, đầu ra và operation• Đầu vào: Dữ liệu được sử dụng để đào tạo và kiểm tra các thuật toán của chúng tôi• Biến: Tensor có thể thay đổi, chủ yếu xác định các tham số của thuật toán của chúng tôi• Đầu ra: Các tensor bất biến lưu trữ cả đầu ra đầu cuối và đầu ra trung gian• Operation: Các phép biến đổi khác nhau cho đầu vào để tạo ra đầu ra mong muốn Định nghĩa đầu vào trong TensorflowCó ba cách khác nhau mà bạn có thể cung cấp dữ liệu cho chương trình TensorFlow:• Tạo dữ liệu dưới dạng mảng Numpy• Tạo dữ liệu dưới dạng tenorflow tensors• Sử dụng API TF.DATA để tạo đường ống đầu vào Định nghĩa biến trong TensorflowCác biến đóng một vai trò quan trọng trong tensorflow. Một biến về cơ bản là một tenxơ với hình dạng cụ thể xác định có bao nhiêu kích thước mà biến sẽ có và kích thước của mỗi chiều. Tuy nhiên, không giống như một tenxơ tenorflow thông thường, các biến có thể thay đổi; nghĩa là giá trị của các biến có thể thay đổi sau khi chúng được xác định. Đây là một thuộc tính lý tưởng để phải thực hiện các tham số của mô hình học tập (ví dụ: trọng lượng mạng thần kinh), trong đó các trọng số thay đổi một chút sau mỗi bước học tập. Ví dụ: nếu bạn xác định một biến có x = tf.varable (0, dtype = tf.int32), bạn có thể thay đổi giá trị của biến đó bằng cách sử dụng hoạt động tenorflow như tf.assign (x, x+1). Tuy nhiên, nếu bạn xác định một tenxơ như x = tf.constant (0, dtype = tf.int32), bạn không thể thay đổi giá trị của tenxơ, như bạn có thể cho một biến. Nó sẽ ở lại 0 cho đến khi kết thúc việc thực hiện chương trình.Tạo biến là khá đơn giản. Trong ví dụ sigmoid của chúng ta, chúng ta đã tạo hai biến, W và b. Khi tạo ra một biến, một vài điều là vô cùng quan trọng. Chúng ta sẽ liệt kê chúng ở đây và thảo luận chi tiết trong các đoạn sau: Hình dạng biến Giá trị ban đầu Kiểu dữ liệu Tên (Tùy chọn)Hình dạng biến là một danh sách của định dạng [x, y, z, …]. Mỗi giá trị trong danh sách cho biết kích thước hoặc trục tương ứng lớn như thế nào. Chẳng hạn, nếu bạn yêu cầu tenxơ 2D với 50 hàng và 10 cột làm biến, hình dạng sẽ bằng [50,10].Kích thước của biến (nghĩa là độ dài của vectơ hình dạng) được công nhận là thứ hạng của tensor trong tenorflow.Tiếp theo, một biến yêu cầu một giá trị ban đầu phải được khởi tạo. TensorFlow cung cấp một số bộ khởi tạo khác nhau, bao gồm các bộ khởi tạo không đổi và bộ khởi tạo phân phối bình thường. Dưới đây là một vài bộ khởi tạo TensorFlow phổ biến mà bạn có thể sử dụng để khởi tạo các biến: tf.initializers.Zeros tf.initializers.Constant tf.initializers.RandomNormal tf.initializers.GlorotUniformHình dạng của biến có thể được cung cấp như là một phần của trình khởi tạo như sau:tf.initializers.RandomUniform(minval=-0.1, maxval=0.1)(shape=[10,5])Kiểu dữ liệu đóng một vai trò quan trọng trong việc xác định kích thước của một biến. Có nhiều loại dữ liệu khác nhau, bao gồm TF.Bool, TF.Uint8, TF.Float32 và TF.INT32. Mỗi loại dữ liệu có một số bit cần thiết để biểu diễn một giá trị duy nhất với loại đó. Ví dụ, tf.uint8 yêu cầu 8 bit, trong khi tf.float32 yêu cầu 32 bit. Đó là thực tế phổ biến để sử dụng các loại dữ liệu tương tự cho các tính toán, vì làm cách khác có thể dẫn đến sự không phù hợp của kiểu dữ liệu. Vì vậy, nếu bạn có hai loại dữ liệu khác nhau cho hai tenxor mà bạn cần chuyển đổi, bạn phải chuyển đổi rõ ràng một tenxơ sang loại tenxơ khác bằng cách sử dụng thao tác tf.cast (…).Hoạt động tf.cast (…) được thiết kế để đối phó với các tình huống như vậy. Ví dụ: nếu bạn có biến X với loại tf.int32, cần được chuyển đổi thành tf.float32, sử dụng tf.cast(x, dtype = tf.float32) để chuyển đổi x thành tf.float32.Cuối cùng, tên của biến sẽ được sử dụng làm ID để xác định biến đó trong biểu đồ. Nếu bạn đã từng trực quan hóa biểu đồ tính toán, biến sẽ xuất hiện bằng đối số được chuyển đến tên từ khóa. Nếu bạn không chỉ định tên, TensorFlow sẽ sử dụng sơ đồ đặt tên mặc định.a = tf.Variable(tf.zeros([5]),name='b')Ở đây, biểu đồ tensorflow sẽ biết biến này bằng tên b chứ không phải a Định nghĩa đầu ra trong TensorflowĐầu ra tenorflow thường là tensor và kết quả của việc chuyển đổi thành đầu vào hoặc một biến hoặc cả hai. Trong ví dụ của chúng ta, h là đầu ra, trong đó h = tf.nn.sigmoid (tf.matmul (x, w) + b). Cũng có thể cung cấp các đầu ra như vậy cho các hoạt động khác, tạo thành một tập hợp các hoạt động chuỗi. Hơn nữa, không nhất thiết phải là hoạt động tensorflow. Bạn cũng có thể sử dụng số học python tiêu chuẩn với tensorflow. Đây là một ví dụ:x = tf.matmul(w,A)y = x + B Định nghĩa operations trong TensorFlowMột hoạt động trong TensorFlow có một hoặc nhiều đầu vào và tạo ra một hoặc nhiều đầu ra. Nếu bạn xem API TensorFlow tại https://www.tensorflow.org/api_docs/python/tf, bạn sẽ thấy TensorFlow có một bộ sưu tập hoạt động lớn. Ở đây, chúng tôi sẽ xem xét một vài trong số các hoạt động vô số tenorflow. Hoạt động so sánhHoạt động so sánh rất hữu ích để so sánh hai tensor. Ví dụ mã sau đây bao gồm một vài hoạt động so sánh hữu ích.import tensorflow as tfx = tf.constant([[1,2],[3,4]], dtype=tf.int32)y = tf.constant([[4,3],[3,2]], dtype=tf.int32)x_equal_y = tf.equal(x, y, name=None)x_less_y = tf.less(x, y, name=None)x_great_equal_y = tf.greater_equal(x, y, name=None)condition = tf.constant([[True,False],[True,False]],dtype=tf.bool)x_cond_y = tf.where(condition, x, y, name=None) Hoạt động toán họcTensorFlow cho phép bạn thực hiện các thao tác toán học trên các tenxơ từ đơn giản đến phức tạp. Bộ hoạt động hoàn chỉnh có sẵn tại https://www.tensorflow.org/versions/r2.0/ api_docs/python/tf/math:x = tf.constant([[1,2],[3,4]], dtype=tf.float32)y = tf.constant([[4,3],[3,2]], dtype=tf.float32)x_add_y = tf.add(x, y)x_mul_y = tf.matmul(x, y)log_x = tf.log(x)x_sum_1 = tf.reduce_sum(x, axis=[1], keepdims=False)x_sum_2 = tf.reduce_sum(x, axis=[0], keepdims=True)data = tf.constant([1,2,3,4,5,6,7,8,9,10], dtype=tf.float32)segment_ids = tf.constant([0,0,0,1,1,2,2,2,2,2 ], dtype=tf.int32)x_seg_sum = tf.segment_sum(data, segment_ids) Cập nhật giá trị trong các tensorMột hoạt động phân tán (scatter operation), đề cập đến việc thay đổi các giá trị tại một số chỉ số nhất định của một tensor, là rất phổ biến trong các vấn đề điện toán khoa học. Chức năng này ban đầu được cung cấp thông qua hàm tf.scatter_nd()Tuy nhiên, trong các phiên bản TensorFlow gần đây, bạn có thể thực hiện các hoạt động phân tán thông qua lập chỉ mục mảng và cắt bằng cú pháp giống như Numpy. Hãy cùng xem một vài ví dụ. Giả sử bạn có TensorFlow biến V, là ma trận [3,2]:v = tf.Variable(tf.constant([[1,9],[3,10],[5,11]],dtype=tf.float32),name='ref') Bạn có thể thay đổi hàng thứ 0 của tenxơ này bằng:v[0].assign([-1, -9])Bạn có thể thay đổi giá trị tại Index [1,1] bằng:v[1,1].assign(-10)Bạn có thể thực hiện cắt hàng với:v[1:,0].assign([-3,-5]) Thu thập các giá trị từ một tenorMột hoạt động tập hợp (gather operation) rất giống với một hoạt động phân tán. Hãy nhớ rằng phân tán là về việc gán các giá trị cho các tensor, trong khi việc thu thập lấy các giá trị của một tensor. Hãy để hiểu điều này thông qua một ví dụ. Giả sử bạn có tenorflow tenor, T:t = tf.constant([[1,9],[3,10],[5,11]],dtype=tf.float32)Bạn có thể có được hàng thứ 0 của T với:t[0].numpy()Bạn cũng có thể thực hiện trượt hàng (row-slicing) với:t[1:,0].numpy()Không giống như hoạt động phân tán, hoạt động tập hợp hoạt động cả trên các cấu trúc TF.Varable và TF.Tensor. 4.Operation liên quan đến mạng thần kinhBây giờ, hãy xem xét một số hoạt động liên quan đến mạng thần kinh hữu ích mà chúng ta sẽ sử dụng rất nhiều trong các chương sau. Các hoạt động mà chúng tôi sẽ thảo luận ở đây bao gồm từ các biến đổi phần tử đơn giản (nghĩa là kích hoạt) đến tính toán các dẫn xuất một phần của một tập hợp các tham số đối với giá trị khác. Chúng ta cũng sẽ triển khai một mạng lưới thần kinh đơn giản. Kích hoạt phi tuyến được sử dụng bởi các mạng thần kinhKích hoạt phi tuyến cho phép các mạng thần kinh hoạt động tốt ở nhiều nhiệm vụ. Thông thường, có một phép biến đổi kích hoạt phi tuyến (nghĩa là lớp kích hoạt) sau mỗi đầu ra lớp trong mạng thần kinh (ngoại trừ lớp cuối cùng). Một phép biến đổi phi tuyến giúp một mạng lưới thần kinh tìm hiểu các mẫu phi tuyến khác nhau có trong dữ liệu. Điều này rất hữu ích cho các vấn đề trong thế giới thực phức tạp, trong đó dữ liệu thường có các mẫu phi tuyến phức tạp hơn, trái ngược với các mẫu tuyến tính. Nếu không dành cho các kích hoạt phi tuyến giữa các lớp, một mạng lưới thần kinh sâu sẽ là một loạt các lớp tuyến tính được xếp chồng lên nhau. Ngoài ra, một tập hợp các lớp tuyến tính về cơ bản có thể được nén vào một lớp tuyến tính lớn hơn.Tóm lại, nếu không cho các kích hoạt phi tuyến, chúng ta không thể tạo ra một mạng lưới thần kinh với nhiều hơn một lớp.Tầm quan trọng của việc kích hoạt phi tuyến thông qua một ví dụ. Đầu tiên, hãy nhớ lại việc tính toán cho các mạng thần kinh mà chúng ta đã thấy trong ví dụ SigMoid.h = sigmoid(W*x)Giả sử một mạng lưới thần kinh ba lớp (có W1, W2 và W3 làm trọng số lớp) trong đó mỗi lớp thực hiện tính toán trước đó; Chúng ta có thể tóm tắt tính toán đầy đủ như sauh = sigmoid(W3*sigmoid(W2*sigmoid(W1*x)))Tuy nhiên, nếu chúng ta loại bỏ kích hoạt phi tuyến (nghĩa là sigmoid), chúng ta sẽ nhận được điều này:h = (W3 * (W2 * (W1 *x))) = (W3*W2*W1)*xVì vậy, không có kích hoạt phi tuyến, ba lớp có thể được đưa xuống một lớp tuyến tính duy nhấtBây giờ chúng tôi sẽ liệt kê hai kích hoạt phi tuyến ( nonlinear activations) thường được sử dụng trong các mạng thần kinh (nói cách khác là SigMoid và Relu) và cách chúng có thể được thực hiện trong TensorFlow# Sigmoid : 1 / (1 + exp(-x))tf.nn.sigmoid(x,name=None)# ReLU activation : max(0,x)tf.nn.relu(x, name=None) Convolution operationMột hoạt động tích chập là một kỹ thuật xử lý tín hiệu được sử dụng rộng rãi. Đối với hình ảnh, tích chập được sử dụng để tạo ra các hiệu ứng khác nhau (như làm mờ) hoặc trích xuất các tính năng (như các cạnh) từ một hình ảnh. Một ví dụ về phát hiện cạnh bằng cách sử dụng tích chập được hiển thị trong Hình dưới. Điều này đạt được bằng cách chuyển một bộ lọc tích chập của hình ảnh để tạo ra một đầu ra khác nhau ở mỗi vị trí. Cụ thể, tại mỗi vị trí, chúng tôi thực hiện phép nhân phần tử của các phần tử trong bộ lọc tích chập với bản vá hình ảnh (image patch) (cùng kích thước với bộ lọc tích chập) trùng với bộ lọc tích chập và lấy tổng của phép nhânSau đây là việc thực hiện hoạt động tích chậpx = tf.constant( [[ [[1],[2],[3],[4]], [[4],[3],[2],[1]], [[5],[6],[7],[8]], [[8],[7],[6],[5]] ]], dtype=tf.float32)x_filter = tf.constant( [ [ [[0.5]],[[1]] ], [ [[0.5]],[[1]] ] ], dtype=tf.float32)x_stride = [1,1,1,1]x_padding = 'VALID'x_conv = tf.nn.conv2d( input=x, filters=x_filter, strides=x_stride, padding=x_padding)Đối với hoạt động tf.nn.conv2d (…), TensorFlow yêu cầu đầu vào, bộ lọc và sải bước ( input, filters, and strides ) có định dạng chính xác. Bây giờ chúng ta sẽ đi qua từng đối số trong tf.conv2d (đầu vào, bộ lọc, sải chân, đệm) ((input, filters, strides, padding)) chi tiết hơn:Input : Đây thường là một tenxơ 4D trong đó các kích thước nên được đặt dưới dạng [batch_size, height, width, channels]: Batch_Size: Đây là lượng dữ liệu (ví dụ: các đầu vào như hình ảnh và từ) trong một lô dữ liệu. Chúng ta thường xử lý dữ liệu theo lô vì các bộ dữ liệu lớn được sử dụng để học. Ở một bước đào tạo nhất định, chúng ta lấy mẫu ngẫu nhiên một lô dữ liệu nhỏ đại diện cho bộ dữ liệu đầy đủ. Và làm điều này cho nhiều bước cho phép chúng ta xấp xỉ bộ dữ liệu đầy đủ khá tốt. Tham số Batch_Size này giống như tham số chúng ta đã thảo luận trong ví dụ đường ống đầu vào TensorFlow. Height and width: Đây là chiều cao và chiều rộng của đầu vào Chanels: Đây là độ sâu của đầu vào (ví dụ: đối với hình ảnh RGB, số lượng kênh sẽ là 3 kênh, một kênh cho mỗi màu).Bộ lọc: Đây là một tenxơ 4D đại diện cho cửa sổ tích chập của hoạt động tích chập. Kích thước bộ lọc phải là [height, width, in_channels, out_channels]: Height and width: Đây là chiều cao và chiều rộng của bộ lọc (thường nhỏ hơn so với đầu vào) in_channels: Đây là số lượng kênh đầu vào cho lớp out_channels: Đây là số lượng kênh được sản xuất trong đầu ra của lớpstrides: Đây là danh sách với bốn yếu tố, trong đó các phần tử là [batch_stride, height_stride, width_stride, channels_stride]. Đối số Strides biểu thị có bao nhiêu phần tử cần bỏ qua trong một dịch chuyển của cửa sổ tích chập trên đầu vào. Thông thường, bạn không phải lo lắng về Batch_Stride và channels_stride. Nếu bạn không hoàn toàn hiểu strides (bước tiến) là gì, bạn có thể sử dụng giá trị mặc định là 1.Padding: Đây có thể là một trong số [‘SAME’, ‘VALID’]. Nó quyết định làm thế nào để xử lý hoạt động tích chập gần ranh giới của đầu vào. Các hoạt động hợp lệ (VALID) thực hiện tích chập mà không cần đệm (padding). Nếu chúng ta kết hợp một đầu vào có độ dài n với một cửa sổ tích chập có kích thước H, điều này sẽ dẫn đến đầu ra có kích thước (N-H+1 &lt;N). Việc giảm kích thước đầu ra có thể hạn chế nghiêm trọng độ sâu của mạng lưới thần kinh. SAME thêm các số 0 đến ranh giới sao cho đầu ra sẽ có cùng chiều cao và chiều rộng với đầu vào.Để hiểu rõ hơn về kích thước bộ lọc, sải chân và đệm (filter size, stride, and padding), tham khảo hình dưới Pooling operationMột hoạt động gộp (pooling operation) hoạt động tương tự như hoạt động tích chập, nhưng đầu ra cuối cùng là khác nhau. Thay vì xuất tổng số nhân của bộ lọc và bản vá hình ảnh, giờ đây chúng ta lấy phần tử tối đa của bản vá hình ảnh cho vị trí đó.x = tf.constant( [[ [[1],[2],[3],[4]], [[4],[3],[2],[1]], [[5],[6],[7],[8]], [[8],[7],[6],[5]] ]], dtype=tf.float32)x_ksize = [1,2,2,1]x_stride = [1,2,2,1]x_padding = 'VALID'x_pool = tf.nn.max_pool2d( input=x, ksize=x_ksize, strides=x_stride, padding=x_padding)# Returns (out) =&gt; [[[[ 4.],[ 4.]],[[ 8.],[ 8.]]]] Định nghĩa mất mátChúng ta biết rằng, đối với một mạng lưới thần kinh để học một cái gì đó hữu ích, một mất mát cần phải được xác định. Sự mất mát thể hiện mức độ gần hoặc xa các dự đoán từ các mục tiêu thực tế. Có một số chức năng để tự động tính toán tổn thất trong tensorflow, hai trong số đó được hiển thị trong mã sau. Hàm tf.nn.l2_loss là mất lỗi bình phương trung bình (mean squared error loss) và tf.nn.softmax_cross_entropy_with_logits là một loại tổn thất khác thực sự mang lại hiệu suất tốt hơn trong các tác vụ phân loại.# Returns half of L2 norm of t given by sum(t**2)/2x = tf.constant([[2,4],[6,8]],dtype=tf.float32)x_hat = tf.constant([[1,2],[3,4]],dtype=tf.float32)# MSE = (1**2 + 2**2 + 3**2 + 4**2)/2 = 15MSE = tf.nn.l2_loss(x-x_hat)y = tf.constant([[1,0],[0,1]],dtype=tf.float32)y_hat = tf.constant([[3,1],[2,5]],dtype=tf.float32)CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat,labels=y)) 5.Keras: API xây dựng mô hình của TensorflowKeras được phát triển như một thư viện riêng biệt cung cấp các khối xây dựng cấp cao để xây dựng các mô hình một cách thuận tiện. Ban đầu nó hỗ trợ nhiều phần mềm (ví dụ: Tensorflow và Theano). Tuy nhiên, Tensorflow có được Keras và bây giờ là một phần không thể thiếu trong TensorFlow để xây dựng các mô hình một cách dễ dàng.Trọng tâm chính của Keras là xây dựng mô hình. Vì vậy, Keras cung cấp một số API khác nhau với mức độ linh hoạt và phức tạp khác nhau. Chọn API phù hợp cho công việc sẽ yêu cầu kiến thức hợp lý về các hạn chế của mỗi API cũng như kinh nghiệm. Các API được cung cấp bởi Keras là: API tuần tự (Sequential API) : API dễ sử dụng nhất. Trong API này, bạn chỉ cần xếp các lớp lên nhau để tạo một mô hình. API chức năng (Functional API) - API chức năng cung cấp tính linh hoạt hơn bằng cách cho phép bạn xác định các mô hình tùy chỉnh có thể có nhiều lớp đầu vào/nhiều lớp đầu ra. API lớp phụ (Sub-classing API) : API lớp phụ cho phép bạn xác định các lớp/ mô hình có thể tái sử dụng tùy chỉnh là các lớp Python. Đây là API linh hoạt nhất, nhưng nó đòi hỏi sự quen thuộc mạnh mẽ với các hoạt động API và tensorflow thô để sử dụng nó một cách chính xácMột trong những khái niệm bẩm sinh nhất trong Keras là một mô hình bao gồm một hoặc nhiều lớp được kết nối theo một cách cụ thể. Ở đây, chúng ta sẽ ngắn gọn về mã trông như thế nào, sử dụng các API khác nhau để phát triển các mô hình. Bạn không mong đợi hiểu đầy đủ mã dưới đây. Thay vào đó, tập trung vào kiểu mã để phát hiện ra bất kỳ sự khác biệt nào giữa ba phương pháp Sequential APIKhi sử dụng API tuần tự, bạn chỉ cần xác định mô hình của mình là danh sách các lớp. Ở đây, phần tử đầu tiên trong danh sách là gần nhất với đầu vào, trong đó phần cuối là lớp đầu ra:model = tf.keras.Sequential([ tf.keras.layers.Dense(500, activation='relu', shape=(784, )), tf.keras.layers.Dense(250, activation='relu'), tf.keras.layers.Dense(10, activation='softmax') ])Trong mã trước, chúng tôi có ba lớp. Lớp đầu tiên có 500 nút đầu ra và lấy một vectơ gồm 784 phần tử làm đầu vào. Lớp thứ hai được tự động kết nối với lớp thứ nhất, trong khi lớp cuối cùng được kết nối với lớp thứ hai. Tất cả các lớp này là các lớp được kết nối đầy đủ, trong đó tất cả các nút đầu vào được kết nối với tất cả các nút đầu ra. Functional APITrong API chức năng, chúng ta làm mọi thứ khác nhau. Trước tiên chúng ta xác định một hoặc nhiều lớp đầu vào và các lớp khác mang tính toán. Sau đó, chúng tôi kết nối các đầu vào với đầu ra, như được hiển thị trong mã sau:inp = tf.keras.layers.Input(shape=(784,))out_1 = tf.keras.layers.Dense(500, activation='relu')(inp)out_2 = tf.keras.layers.Dense(250, activation='relu')(out_1)out = tf.keras.layers.Dense(10, activation='softmax')(out_2)model = tf.keras.models.Model(inputs=inp, outputs=out)Trong mã, chúng ta bắt đầu với một lớp đầu vào chấp nhận vectơ dài 784 phần tử. Đầu vào được truyền đến một lớp dày đặc có 500 nút. Đầu ra của lớp đó được gán cho out_1. Sau đó out_1 được chuyển cho một lớp dày đặc khác, xuất ra out_2. Tiếp theo, một lớp dày đặc với 10 nút đầu ra đầu ra cuối cùng. Cuối cùng, mô hình được định nghĩa là đối tượng tf.keras.models.Model có hai đối số: inputs - một hoặc nhiều lớp đầu vào outputs - một hoặc nhiều đầu ra được tạo bởi bất kỳ tf.keras.layers loại đối tượngMô hình giống hệt với những gì được xác định trong phần trước. Một trong những lợi ích của API chức năng là bạn có thể tạo các mô hình phức tạp hơn nhiều vì bạn không bị ràng buộc để có các lớp như một danh sách. Vì sự tự do này, bạn có thể có nhiều đầu vào kết nối với nhiều lớp theo nhiều cách khác nhau và có khả năng tạo ra nhiều đầu ra. Sub-classing APICuối cùng, chúng ta sẽ sử dụng API lớp phụ để xác định mô hình. Với lớp phụ, bạn xác định mô hình của mình là một đối tượng Python kế thừa từ đối tượng cơ sở tf.keras.model. Khi sử dụng lớp phụ, bạn cần xác định hai hàm quan trọng: __init __ (), sẽ chỉ định bất kỳ tham số, lớp đặc biệt nào, và do đó cần thiết để thực hiện thành công các tính toán và hàm call() xác định các tính toán cần phải xảy ra trong mô hình:class MyModel(tf.keras.Model): def __init__(self, num_classes): super().__init__() self.hidden1_layer = tf.keras.layers.Dense(500, activation='relu') self.hidden2_layer = tf.keras.layers.Dense(250, activation='relu') self.final_layer = tf.keras.layers.Dense(num_classes, activation='softmax') def call(self, inputs): h = self.hidden1_layer(inputs) h = self.hidden2_layer(h) y = self.final_layer(h) return ymodel = MyModel(num_classes=10)Ở đây, bạn có thể thấy rằng mô hình của chúng ta có ba lớp, giống như tất cả các mô hình trước đó chúng ta đã xác định. Tiếp theo, hàm call() xác định cách các lớp này kết nối để tạo ra đầu ra cuối cùng. API lớp phụ được coi là khó khăn nhất để làm chủ, chủ yếu là do sự tự do. Tuy nhiên, phần thưởng là rất lớn khi bạn tìm hiểu API vì nó cho phép bạn xác định các mô hình/lớp rất phức tạp là các tính toán đơn vị có thể được sử dụng lại sau đó. Bây giờ bạn đã hiểu cách mỗi API hoạt động, hãy để thực hiện một mạng lưới thần kinh bằng cách sử dụng Keras và đào tạo nó trên một bộ dữ liệu. 6.Thực hiện mạng neural network đầu tiên của chúng taMột trong những bước đệm để giới thiệu các mạng thần kinh là triển khai một mạng lưới thần kinh có khả năng phân loại các chữ số. Đối với nhiệm vụ này, chúng tôi sẽ sử dụng bộ dữ liệu MNIST nổi tiếng được cung cấp tại http://yann.lecun.com/exdb/mnist/.Bạn có thể cảm thấy một chút hoài nghi về việc chúng ta sử dụng nhiệm vụ tầm nhìn máy tính hơn là một nhiệm vụ NLP. Tuy nhiên, các nhiệm vụ tầm nhìn có thể được thực hiện với ít tiền xử lý hơn và dễ hiểu.Vì đây là cuộc gặp gỡ đầu tiên của chúng ta với các mạng thần kinh, chúng ta sẽ thấy cách thực hiện mô hình này bằng cách sử dụng Keras. Keras là mô hình con cấp cao cung cấp một lớp trừu tượng qua tensorflow. Do đó, bạn có thể triển khai các mạng thần kinh với ít nỗ lực hơn với Keras hơn là sử dụng các hoạt động thô của TensorFlow. Chuẩn bị dữ liệuĐầu tiên, chúng ta cần tải xuống bộ dữ liệu. TensorFlow cung cấp các chức năng thuận tiện để tải xuống dữ liệu và MNIST là một trong những bộ dữ liệu được hỗ trợ đó. Chúng tôi sẽ thực hiện bốn bước quan trọng trong quá trình chuẩn bị dữ liệu: Tải xuống dữ liệu và lưu trữ nó dưới dạng các đối tượng numpy.ndarray. Định hình lại các hình ảnh để hình ảnh thang độ xám 2D trong bộ dữ liệu sẽ được chuyển đổi thành vectơ 1D. Tiêu chuẩn hóa các hình ảnh có trung bình không và đơn vị (zero-mean and unit-variance) (còn được gọi là làm trắng). One-hot encoding nhãn lớp số nguyên. Mã hóa một lần đề cập đến quá trình biểu diễn nhãn lớp số nguyên dưới dạng vectơ. Ví dụ: nếu bạn có 10 lớp và nhãn lớp 3 (trong đó các nhãn nằm trong khoảng từ 0-9), vectơ được mã hóa một lần nóng (One-hot encoding) của bạn sẽ là [0, 0, 0, 1, 0, 0, 0, 0, 0, 0 , 0].Mã sau đây thực hiện các chức năng này cho chúng tôi:os.makedirs('data', exist_ok=True)(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data( path=os.path.join(os.getcwd(), 'data', 'mnist.npz'))# Reshaping x_train and x_test tensors so that each image is represented# as a 1D vectorx_train = x_train.reshape(x_train.shape[0], -1)x_test = x_test.reshape(x_test.shape[0], -1)# Standardizing x_train and x_test tensorsx_train = ( x_train - np.mean(x_train, axis=1, keepdims=True))/np.std(x_train, axis=1, keepdims=True)x_test = ( x_test - np.mean(x_test, axis=1, keepdims=True))/np.std(x_test, axis=1, keepdims=True)# One hot encoding y_train and y_testy_onehot_train = np.zeros((y_train.shape[0], num_labels),dtype=np.float32)y_onehot_train[np.arange(y_train.shape[0]), y_train] = 1.0y_onehot_test = np.zeros((y_test.shape[0], num_labels), dtype=np.float32)y_onehot_test[np.arange(y_test.shape[0]), y_test] = 1.0Bạn có thể thấy rằng chúng ta đang sử dụng chức năng tf.keras.datasets.mnist.load_data() do TensorFlow cung cấp để tải xuống dữ liệu đào tạo và kiểm tra. Điều này sẽ cung cấp bốn tensors đầu ra- x_train - Một tensor có kích thước 60000 x 28 x 28 trong đó mỗi hình ảnh là 28 x 28- y_train - Một vectơ có kích thước 60000, trong đó mỗi phần tử là một nhãn lớp từ 0-9 - x_test - Tensor có kích thước 10000 x 28 x 28 - y_test - Vectơ có kích thước 10000Khi dữ liệu được tải xuống, chúng ta định hình lại hình ảnh có kích thước 28 x 28 thành một vectơ 1D. Điều này là do chúng ta sẽ triển khai một mạng lưới thần kinh được kết nối đầy đủ. Các mạng thần kinh được kết nối đầy đủ lấy một vectơ 1D làm đầu vào. Do đó, tất cả các pixel trong hình ảnh sẽ được sắp xếp như một chuỗi các pixel để đưa vào mô hình. Cuối cùng, nếu bạn nhìn vào phạm vi của các giá trị có trong các tensor X_Train và X_Test, chúng sẽ nằm trong phạm vi 0-255 (phạm vi thang độ xám điển hình). Chúng ta sẽ đưa các giá trị này vào phạm vi phương sai đơn vị trung bình bằng không bằng cách trừ trung bình của mỗi hình ảnh và chia cho độ lệch chuẩn. Triển khai neural network với KerasMạng thần kinh được kết nối đầy đủ với 3 lớp có 500, 250 và 10 nút tương ứng. Hai lớp đầu tiên sẽ sử dụng kích hoạt Relu, trong khi lớp cuối cùng sử dụng SoftMax. Để thực hiện điều này, chúng tôi sẽ sử dụng các API KERAS đơn giản nhất có sẵn cho chúng ta - API tuần tự.model = tf.keras.Sequential([ tf.keras.layers.Dense(500, activation='relu'), tf.keras.layers.Dense(250, activation='relu'), tf.keras.layers.Dense(10, activation='softmax') ])Bạn có thể thấy rằng tất cả những gì nó cần là một dòng duy nhất trong API tuần tự Keras để xác định mô hình mà chúng ta vừa xác định. Keras cung cấp nhiều loại lớp khác nhau. Bạn có thể thấy danh sách đầy đủ các lớp có sẵn cho bạn tại https://www.tensorflow.org/api_docs/python/tf/keras/layers. Đối với một mạng được kết nối đầy đủ, chúng ta chỉ cần các lớp dày đặc bắt chước các tính toán của một lớp ẩn trong một mạng được kết nối đầy đủ. Với mô hình được xác định, bạn cần biên dịch mô hình này với chức năng tổn thất phù hợp, trình tối ưu hóa và hiệu suất:optimizer = tf.keras.optimizers.RMSprop()loss_fn = tf.keras.losses.CategoricalCrossentropy()model.compile(optimizer=optimizer, loss=loss_fn, metrics=['acc'])Với mô hình được xác định và biên dịch, giờ đây chúng ta có thể đào tạo mô hình của mình trên dữ liệu đã chuẩn bị. Training the modelĐào tạo một mô hình không thể dễ dàng hơn với Keras. Khi dữ liệu được chuẩn bị, tất cả những gì bạn cần làm là gọi hàm model.fit () với các đối số cần thiết:batch_size = 100num_epochs = 10train_history = model.fit( x=x_train, y=y_onehot_train, batch_size=batch_size, epochs= num_epochs, validation_split=0.2)model.fit () chấp nhận một số đối số quan trọng. Chúng ta sẽ đi qua chúng chi tiết hơn ở đây: X - Một tenxơ đầu vào. Trong trường hợp của chúng ta, đây là một tenxơ có kích thước 60000 x 784. Y - Nhãn được mã hóa một lần nóng (one-hot encoded). Trong trường hợp của chúng ta, đây là một tenxơ có kích thước 60000 x 10. batch_size - Các mô hình học tập sâu được đào tạo với các lô dữ liệu (nói cách khác, một cách ngẫu nhiên) trái ngược với việc cung cấp cho bộ dữ liệu đầy đủ cùng một lúc. Kích thước lô xác định có bao nhiêu ví dụ được bao gồm trong một lô. Kích thước lô càng lớn, độ chính xác của mô hình của bạn sẽ càng tốt. Epochs - Các mô hình học tập sâu lặp lại thông qua bộ dữ liệu theo các lô nhiều lần. Số lần lặp lại thông qua bộ dữ liệu được gọi là số lượng kỷ nguyên. Trong ví dụ của chúng ta, điều này được đặt thành 10. validation_split - Khi đào tạo các mô hình học tập sâu, một bộ xác nhận được sử dụng để theo dõi hiệu suất, trong đó bộ xác thực hoạt động như một proxy cho hiệu suất trong thế giới thực. validation_split xác định số lượng bộ dữ liệu đầy đủ sẽ được sử dụng làm tập hợp con xác thực. Trong ví dụ của chúng ta, điều này được đặt thành 20% tổng kích thước tập dữ liệuỞ đây, những gì training loss và validation accuracy trông giống như số lượng kỷ nguyên mà chúng ta đã đào tạo mô hìnhTiếp theo là kiểm tra mô hình của chúng tôi trên một số dữ liệu chưa từng thấy Kiểm tra modelKiểm tra mô hình cũng đơn giản. Trong quá trình thử nghiệm, chúng ta đo lường sự mất mát và độ chính xác của mô hình trên bộ dữ liệu thử nghiệm. Để đánh giá mô hình trên bộ dữ liệu, các mô hình Keras cung cấp chức năng thuận tiện gọi là evaluate():test_res = model.evaluate( x=x_test, y=y_onehot_test, batch_size=batch_size)Các đối số được mong đợi bởi hàm evaluate() đã được đề cập trong quá trình thảo luận của chúng ta về model.fit (): X - một tenxơ đầu vào. Trong trường hợp của chúng ta, đây là một tenxơ có kích thước 10000 x 784. Y - Nhãn được mã hóa một lần nóng. Trong trường hợp của chúng ta, đây là một tensor kích thước 10000 x 10. Batch_size - Kích thước lô xác định số lượng ví dụ được bao gồm trong một lô. Kích thước lô càng lớn thì độ chính xác của mô hình của bạn sẽ càng tốtBạn sẽ bị loss 0,138 và độ chính xác là 98%. Bạn sẽ không nhận được các giá trị chính xác giống nhau do sự ngẫu nhiên khác nhau trong mô hình, cũng như trong quá trình đào tạo III.Word2vec 1.Giới thiệuWord2vec là một mô hình đơn giản và nổi tiếng giúp tạo ra các biểu diễn embedding của từ trong một không gian có số chiều thấp hơn nhiều lần so với số từ trong từ điển.Ý tưởng cơ bản của word2vec có thể được gói gọn trong các ý sau: Hai từ xuất hiện trong những văn cảnh giống nhau thường có ý nghĩa gần với nhau. Ta có thể đoán được một từ nếu biết các từ xung quanh nó trong câu. Ví dụ, với câu “Hà Nội là … của Việt Nam” thì từ trong dấu ba chấm khả năng cao là “thủ đô”. Với câu hoàn chỉnh “Hà Nội là thủ đô của Việt Nam”, mô hình word2vec sẽ xây dựng ra embeding của các từ sao cho xác suất để từ trong dấu ba chấm là “thủ đô” là cao nhất. 2.Một vài định nghĩaTrong ví dụ trên đây, từ “thủ đô” đang được xét và được gọi là target word hay từ đích. Những từ xung quanh nó được gọi là context words hay từ ngữ cảnh. Với mỗi từ đích trong một câu của cơ sở dữ liệu, các từ ngữ cảnh được định nghĩa là các từ trong cùng câu có vị trí cách từ đích một khoảng không quá C/2 với C là một số tự nhiên dương. Như vậy, với mỗi từ đích, ta sẽ có một bộ không quá C từ ngữ cảnh.Xét ví dụ sau đây với câu tiếng Anh: “The quick brown fox jump over the lazy dog” với C=4.Khi “the” là từ đích, ta có cặp dữ liệu huấn luyện là (the, quick) và (the, brown). Khi “brown” là từ đích, ta có cặp dữ liệu huấn luyện là (brown, the), (brown, quick), (brown, fox) và (brown, jumps).Word2vec định nghĩa hai embedding vector cùng chiều cho mỗi từ w trong từ điển. Khi nó là một từ đích, embedding vector của nó là u; khi nó là một từ ngữ cảnh, embedding của nó là v. Sở dĩ ta cần hai embedding khác nhau vì ý nghĩa của từ đó khi nó là từ đích và từ ngữ cảnh là khác nhau. Tương ứng với đó, ta có hai ma trận embedding U và V cho các từ đích và các từ ngữ cảnh.Có hai cách khác nhau xây dựng mô hình word2vec: Skip-gram: Dự đoán những từ ngữ cảnh nếu biết trước từ đích. CBOW (Continuous Bag of Words): Dựa vào những từ ngữ cảnh để dự đoán từ đích. Mỗi cách có những ưu nhược điểm khác nhau và áp dụng với những loại dữ liệu khác nhau. 3.Skip-gramMô hình skip-gram liên tục học bằng cách dự đoán các từ xung quanh được đưa ra một từ hiện tại. Nói cách khác, Mô hình Skip-Gram liên tục dự đoán các từ trong một phạm vi nhất định trước và sau từ hiện tại trong cùng một câu.skip-gram dự đoán ngữ cảnh hoặc các từ lân cận cho một từ nhất định. Mô hình Skip-Gram được đào tạo trên các cặp n-gram (target_word, context_word) với mã thông báo là 1 và 0. Mã thông báo chỉ định xem context_words đến từ cùng một cửa sổ hay được tạo ngẫu nhiên. Cặp có mã thông báo 0 bị bỏ qua. Mã triển khai mô hình Skip-GramCác bước cần tuân theo: Xây dựng vốn từ vựng corpus Xây dựng trình tạo skip-gram [(mục tiêu, ngữ cảnh), mức độ liên quan] Xây dựng kiến trúc mô hình skip-gram Đào tạo mô hình Nhận nhúng Word 1. Xây dựng vốn từ vựng corpus:Bước thiết yếu trong khi xây dựng bất kỳ mô hình dựa trên NLP nào là tạo ra một kho tài liệu trong đó chúng tôi trích xuất từng từ duy nhất từ vựng và gán một số nhận dạng duy nhất cho nó.Kho tư liệu chúng ta đang sử dụng là ‘The King James Version of the Bible’, từ Dự án Gutenberg, có sẵn miễn phí thông qua mô hình corpus trong nltk.from nltk.corpus import gutenberg # to get bible corpusfrom string import punctuation # to remove punctuation from corpusimport nltk import numpy as npfrom keras.preprocessing import textfrom keras.preprocessing.sequence import skipgrams from keras.layers import *from keras.layers.core import Dense, Reshapefrom keras.layers.embeddings import Embeddingfrom keras.models import Model,Sequential nltk.download('gutenberg')nltk.download('punkt')nltk.download('stopwords')# english là ngôn ngữ bạn chọnstop_words = nltk.corpus.stopwords.words('english')Quá trình chuyển đổi dữ liệu sang một thứ mà máy tính có thể hiểu được gọi là tiền xử lý. Một trong những hình thức xử lý trước chính là lọc ra những dữ liệu vô dụng. Trong xử lý ngôn ngữ tự nhiên, những từ vô ích (dữ liệu), được gọi là những từ dừng(stop words). Từ dừng là một từ thường được sử dụng (chẳng hạn như “the”, “a”, “an”, “in”) mà công cụ tìm kiếm đã được lập trình để bỏ qua.Chúng ta sử dụng chức năng do người dùng xác định để xử lý sơ bộ văn bản giúp loại bỏ các khoảng trắng, chữ số, từ dừng và viết tắt thân văn bảnimport rebible = gutenberg.sents(\"bible-kjv.txt\")remove_terms = punctuation + '0123456789'wpt = nltk.WordPunctTokenizer()def normalize_document(doc): # lower case and remove special characters\\whitespaces doc = re.sub(r'[^a-zA-Z\\s]', '', doc,re.I|re.A) doc = doc.lower() doc = doc.strip() # tokenize document tokens = wpt.tokenize(doc) # filter stopwords out of document filtered_tokens = [token for token in tokens if token not in stop_words] # re-create document from filtered tokens doc = ' '.join(filtered_tokens) return docnormalize_corpus = np.vectorize(normalize_document)" }, { "title": "Biến tool của bạn thành package có thể cài đặt qua pip", "url": "/posts/Bi%E1%BA%BFn-tool-c%E1%BB%A7a-b%E1%BA%A1n-th%C3%A0nh-package-c%C3%B3-th%E1%BB%83-c%C3%A0i-%C4%91%E1%BA%B7t-qua-pip/", "categories": "Python, Pypi", "tags": "pypi, package, github", "date": "2022-09-23 00:00:00 +0700", "snippet": "Như tiêu đề bài viết, bài post này ghi lại quá trình tôi xuất bản một công cụ cá nhân bằng python code đến cộng đồng, mọi người có thể sử dụng nó với việc “pip install package” .Vui lòng đợi, tôi sẽ hoàn thiện nó ngay khi có đủ thời gian.Bạn có thể tìm thấy nó ở đây https://pypi.org/project/test-package-tlqb/" }, { "title": "Phân loại âm thanh (P2)", "url": "/posts/Ph%C3%A2n-lo%E1%BA%A1i-%C3%A2m-thanh-(P2)/", "categories": "Python, Machine-learning", "tags": "ML, AI", "date": "2022-09-22 00:00:00 +0700", "snippet": "Khái niệm cơ bản về phân loại âm thanhDạng sóng (Waveform)Âm thanh là những dao động do một vật tạo ra khi các phần tử không khí xung quanh dao động. Âm thanh là một sóng cơ học, nơi năng lượng được truyền từ nguồn này sang nguồn khác. Dạng sóng là một biểu diễn giản đồ giúp chúng ta phân tích sự dịch chuyển của sóng âm thanh theo thời gian, cùng với một số tham số thiết yếu khác cần thiết cho một nhiệm vụ cụ thể.Mặt khác, tần số ở dạng sóng là đại diện của số lần một dạng sóng lặp lại chính nó trong khoảng thời gian một giây. Đỉnh của dạng sóng ở trên cùng được gọi là đỉnh, trong khi điểm dưới cùng được gọi là đáy. Biên độ là khoảng cách từ đường tâm đến đỉnh của máng hoặc đáy của đỉnh.Quang phổ (Spectrograms)Quang phổ là một biểu diễn trực quan của phổ tần số của một tín hiệu khi nó thay đổi theo thời gian. Khi được áp dụng cho một tín hiệu âm thanh , các bản ghi quang phổ đôi khi được gọi là bản ghi âmDự án Phân loại Âm thanhBằng cách chuyển đổi dạng sóng thô của dữ liệu âm thanh thành dạng quang phổ, chúng ta có thể chuyển nó qua các mô hình học sâu để giải thích và phân tích dữ liệu.Trong dự án này, mục tiêu là thu lại âm thanh phát ra từ một con chim. Khi đã thu được thành công dạng sóng, chúng ta có thể tiến hành chuyển dạng sóng này thành một biểu đồ quang phổ, đây là biểu diễn trực quan của dạng sóng có sẵn. Vì những hình ảnh phổ này là hình ảnh trực quan, chúng ta có thể sử dụng mạng nơ-ron tích tụ để phân tích chúng cho phù hợp bằng cách tạo mô hình học sâu để tính toán kết quả phân loại nhị phânThực hiện dự án Nhận dạng và Phân loại Âm thanh với Học sâu:Mục tiêu của dự án là đọc âm thanh đến từ một khu rừng và giải thích xem dữ liệu nhận được thuộc về một loài chim cụ thể (chim mũ lưỡi trai) hay là một số tiếng ồn khác mà ta không thực sự quan tâm đến việc ghi nhận.Thư viện TensorFlow-io, sẽ cấp cho ta quyền truy cập vào các hệ thống tệp và định dạng tệp không có sẵn trong hỗ trợ tích hợp của TensorFlow!pip install tensorflow-io[tensorflow]Import các thư viện cần thiếtimport tensorflow as tf from tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Conv2D, Dense, Flattenimport tensorflow_io as tfiofrom matplotlib import pyplot as pltimport osTải dataset: https://www.kaggle.com/datasets/kenjee/z-by-hp-unlocked-challenge-3-signal-processingCó ba thư mục trong thư mục data. Ba thư mục cụ thể là các bản ghi âm trong rừng chứa một đoạn clip dài ba phút về âm thanh được tạo ra trong rừng, đoạn clip dài ba giây về các bản ghi âm của chim Capuchin và đoạn ghi âm dài ba giây về âm thanh không do chim Capuchin tạo raCAPUCHIN_FILE = os.path.join('data', 'Parsed_Capuchinbird_Clips', 'XC3776-3.wav')NOT_CAPUCHIN_FILE = os.path.join('data', 'Parsed_Not_Capuchinbird_Clips', 'afternoon-birds-song-in-forest-0.wav')Hàm được xác định trong đoạn mã dưới đây sẽ cho phép ta đọc dữ liệu và chuyển đổi nó thành một kênh đơn (hoặc đơn) để phân tích dễ dàng hơnHàm tf.squeeze() trả về một tensor có cùng giá trị với đối số đầu tiên của nó, nhưng có hình dạng khác. Nó loại bỏ các thứ nguyên có kích thước là mộtdef load_wav_16k_mono(filename): # Load encoded wav file file_contents = tf.io.read_file(filename) # Decode wav (tensors by channels) wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1) # Removes trailing axis wav = tf.squeeze(wav, axis=-1) sample_rate = tf.cast(sample_rate, dtype=tf.int64) # Goes from 44100Hz to 16000hz - amplitude of the audio signal wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000) return wavChuẩn bị tập dữ liệuNhãn 1: tiếng chim CapuchinNhãn 0: tín hiệu âm thanh nhiễutf.data.Dataset.list_file: để quét tất cả các phần tử thư mục data# Defining the positive and negative pathsPOS = os.path.join('data', 'Parsed_Capuchinbird_Clips/*.wav')NEG = os.path.join('data', 'Parsed_Not_Capuchinbird_Clips/*.wav')# Creating the Datasetspos = tf.data.Dataset.list_files(POS)neg = tf.data.Dataset.list_files(NEG)# Adding labelspositives = tf.data.Dataset.zip((pos, tf.data.Dataset.from_tensor_slices(tf.ones(len(pos)))))negatives = tf.data.Dataset.zip((neg, tf.data.Dataset.from_tensor_slices(tf.zeros(len(neg)))))data = positives.concatenate(negatives)Phân tích bước sóng trung bình của chim Capuchin# Analyzing the average wavelength of a Capuchin birdlengths = []for file in os.listdir(os.path.join('data', 'Parsed_Capuchinbird_Clips')): tensor_wave = load_wav_16k_mono(os.path.join('data', 'Parsed_Capuchinbird_Clips', file)) lengths.append(len(tensor_wave))from statistics import meanmean(lengths)wav = load_wav_16k_mono('data/Parsed_Capuchinbird_Clips/XC22397-2.wav')wav = wav[:48000]zero_padding = tf.zeros([48000] - tf.shape(wav), dtype=tf.float32)wav = tf.concat([zero_padding, wav],0)spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)spectrogram = tf.abs(spectrogram)spectrogram = tf.expand_dims(spectrogram, axis=2)Thu thập tất cả các dạng sóng và tính toán Tín hiệu Biến đổi Fourier trong thời gian ngắn với thư viện TensorFlow để có được một biểu diễn trực quandef preprocess(file_path, label): wav = load_wav_16k_mono(file_path) wav = wav[:48000] zero_padding = tf.zeros([48000] - tf.shape(wav), dtype=tf.float32) wav = tf.concat([zero_padding, wav],0) spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32) spectrogram = tf.abs(spectrogram) spectrogram = tf.expand_dims(spectrogram, axis=2) return spectrogram, label filepath, label = positives.shuffle(buffer_size=10000).as_numpy_iterator().next()spectrogram, label = preprocess(filepath, label)Xây dựng mô hình học sâuTải các phần tử dữ liệu chương trình quang phổ thu được từ chức năng bước tiền xử lý và lưu vào bộ nhớ cache và xáo trộn dữ liệu này bằng cách sử dụng các chức năng có sẵn của TensorFlow, cũng như tạo kích thước lô gồm mười sáu để tải các phần tử dữ liệu cho phù hợp.tf.data.Dataset.map(): được sử dụng để chuyển đổi các mục trong tập dữ liệuSử dụng tf.data.Dataset.cache()thực sự không phải là một lựa chọn tốt vì nó sẽ lưu toàn bộ tập dữ liệu vào bộ nhớ, điều này gây mất thời gian và có thể làm tràn bộ nhớ của bạnshuffle( buffer_size, seed=None, reshuffle_each_iteration=None) Phương pháp xáo trộn các mẫu trong tập dữ liệu. Buffer_size là số lượng mẫu được lấy ngẫu nhiên và trả về dưới dạng tf.Dataset.batch(batch_size,drop_remainder=False)Tạo các lô của tập dữ liệu với kích thước lô được cung cấp vì batch_size cũng là độ dài của các lô.Ngay sau khi tất cả các mục được đọc từ tập dữ liệu và bạn cố đọc phần tử tiếp theo, tập dữ liệu sẽ gặp lỗi. Đó là nơi ds.repeat() phát huy tác dụng. Nó sẽ khởi tạo lại tập dữ liệuHầu hết các đường ống đầu vào của tập dữ liệu phải kết thúc bằng một lệnh gọi đến prefetch. Điều này cho phép các phần tử sau này được chuẩn bị trong khi phần tử hiện tại đang được xử lý. Điều này thường cải thiện độ trễ và thông lượng, với chi phí sử dụng bộ nhớ bổ sung để lưu trữ các phần tử tìm nạp trước.# Creating a Tensorflow Data Pipelinedata = data.map(preprocess)data = data.cache()data = data.shuffle(buffer_size=1000)data = data.batch(16)data = data.prefetch(8)Trước khi tiến hành xây dựng mô hình học sâu, ta có thể tạo phân vùng cho các mẫu thử nghiệm và đào tạo, như được hiển thị trong đoạn mã bên dưới.# Split into Training and Testing Partitionstrain = data.take(36)test = data.skip(36).take(15)# độ dài của tập dữ liệutf.data.experimental.cardinality(train)# hiển thị shape của tf.Tensorfor i in train: print(i)Xây dựng mô hình kiểu tuần tựXây dựng hai khối lớp chập với mười sáu bộ lọc và kích thước hạt nhân là (3, 3)Chức năng kích hoạt ReLU được sử dụng trong việc xây dựng các lớp tích tụ.Sau đó, chúng ta có thể tiến hành làm phẳng đầu ra thu được từ các lớp tích tụ để làm cho nó phù hợp cho quá trình xử lý tiếp theo.Cuối cùng, chúng ta có thể thêm các lớp được kết nối đầy đủ với chức năng kích hoạt Sigmoid với một nút đầu ra để nhận đầu ra phân loại nhị phân.model = Sequential()model.add(Conv2D(16, (3,3), activation='relu', input_shape=(1491, 257,1)))model.add(Conv2D(16, (3,3), activation='relu'))model.add(Flatten())# model.add(Dense(128, activation='relu'))model.add(Dense(1, activation='sigmoid'))model.summary()Để biên dịch mô hình, chúng ta có thể sử dụng trình tối ưu hóa Adam, hàm mất mát entropy chéo nhị phân để phân loại nhị phân và xác định một số số liệu thu hồi và độ chính xác bổ sung cho phân tích mô hình.# Compiling and fitting the modelmodel.compile('Adam', loss='BinaryCrossentropy', metrics=[tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])model.fit(train, epochs=4, validation_data=test)model.save('mymodel.h5')from keras.models import load_modelmodel = load_model('mymodel.h5')Đưa ra các Dự đoán# Prediction for a single batchX_test, y_test = test.as_numpy_iterator().next()yhat = model.predict(X_test)# converting logits to classesyhat = [1 if prediction &gt; 0.5 else 0 for prediction in yhat]Hàm bên dưới nhận đầu vào định dạng mp3 và chuyển đổi chúng thành tensor. Sau đó, ta tính giá trị trung bình của đầu vào đa kênh để chuyển nó thành kênh đơn và thu được tín hiệu tần số mong muốn.def load_mp3_16k_mono(filename): \"\"\" Load an audio file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\" res = tfio.audio.AudioIOTensor(filename) # Convert to tensor and combine channels tensor = res.to_tensor() tensor = tf.math.reduce_sum(tensor, axis=1) / 2 # Extract sample rate and cast sample_rate = res.rate sample_rate = tf.cast(sample_rate, dtype=tf.int64) # Resample to 16 kHz wav = tfio.audio.resample(tensor, rate_in=sample_rate, rate_out=16000) return wav mp3 = os.path.join('data', 'Forest Recordings', 'recording_00.mp3')wav = load_mp3_16k_mono(mp3)audio_slices = tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length=48000, sequence_stride=48000, batch_size=1)samples, index = audio_slices.as_numpy_iterator().next()Ta sẽ xây dựng một hàm giúp tách các phân đoạn riêng lẻ thành các bản đồ quang phổ có cửa sổ để tính toán thêm. Ta sẽ ánh xạ dữ liệu phù hợp và tạo các lát cắt thích hợp để đưa ra các dự đoán cần thiết# Build Function to Convert Clips into Windowed Spectrogramsdef preprocess_mp3(sample, index): sample = sample[0] zero_padding = tf.zeros([48000] - tf.shape(sample), dtype=tf.float32) wav = tf.concat([zero_padding, sample],0) spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32) spectrogram = tf.abs(spectrogram) spectrogram = tf.expand_dims(spectrogram, axis=2) return spectrogramaudio_slices = tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length=16000, sequence_stride=16000, batch_size=1)audio_slices = audio_slices.map(preprocess_mp3)#audio_slices = audio_slices.batch(64)audio_slices = audio_slices.batch(32)yhat = model.predict(audio_slices)yhat = [1 if prediction &gt; 0.5 else 0 for prediction in yhat]Ta sẽ chạy quy trình sau cho tất cả các tệp trong bản ghi rừng và thu được tổng kết quả được tính toán. Kết quả sẽ chứa các đoạn số không và các đoạn trong đó tổng số các số đó được xuất ra để tính điểm tổng thể của các đoạn clip. Chúng ta có thể tìm ra tổng số tiếng kêu của chim Capuchin trong bản ghi âm theo yêu cầu của dự ánresults = {}class_preds = {}for file in os.listdir(os.path.join('data', 'Forest Recordings')): FILEPATH = os.path.join('data','Forest Recordings', file) wav = load_mp3_16k_mono(FILEPATH) audio_slices = tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length=48000, sequence_stride=48000, batch_size=1) audio_slices = audio_slices.map(preprocess_mp3) audio_slices = audio_slices.batch(64) yhat = model.predict(audio_slices) results[file] = yhat for file, logits in results.items(): class_preds[file] = [1 if prediction &gt; 0.99 else 0 for prediction in logits]class_preds" }, { "title": "Tôi đã tạo ra một blog như thế nào với github page", "url": "/posts/T%C3%B4i-%C4%91%C3%A3-t%E1%BA%A1o-ra-m%E1%BB%99t-blog-nh%C6%B0-th%E1%BA%BF-n%C3%A0o-v%E1%BB%9Bi-github-page/", "categories": "web, blog", "tags": "github page, web, blog", "date": "2022-09-21 00:00:00 +0700", "snippet": "Như tiêu đề bài viết, bài post ghi lại quá trình tôi tạo ra một blog cá nhân với github page và jekyll.Sơ lượcGiới thiệuNhư các bạn đã biết website còn gọi là trang web là tập hợp các trang chứa thông tin bao gồm văn bản, hình ảnh, video,… nằm trên một domain, được lưu trữ trên máy chủ web, ví dụ về website như (vietnix.vn, google.com, facebook.com).Website được xem là công cụ hỗ trợ đắc lực cho hoạt động Marketing online, góp phần quảng bá rộng rãi hình ảnh doanh nghiệp, quảng cáo sản phẩm, dịch vụ đến khách hàng nhanh chóng giúp xây dựng thương hiệu, tạo dựng sự uy tín, đồng thời nâng cao sức mạnh cạnh tranh cho các đơn vị kinh doanh trên thị trường.Hiện tại có rất nhiều cách để tạo 1 trang web (cho người không biết lập trình cho đến các ;lập trình viên chuyên nghiệp) như Wordpress,Wix,Python django, Ruby on rails … Trong bài viết này, tôi muốn giới thiệu đến các bạn một cách đơn giản hơn (cho các bạn biết một tý về lập trình) đó là blog với github page và jekyll.Mục tiêu bài viết Cung cấp kiến thức đầy đủ về việc thiết lập cơ bản cho đến tạo ra một web với đầy đủ chức năng. Ghi lại quá trình làm việc và học tập trên nền tảng này. Các liên kết tham khảo https://pages.github.com/ https://viblo.asia/p/dung-jekyll-travis-va-github-pages-de-tao-ra-muon-van-trang-web-de-dang-Qbq5QWp3ZD8 https://github.com/cotes2020/chirpy-starter Tạo blog Với các trang GitHub, GitHub cho phép bạn lưu trữ một trang web từ kho lưu trữ của mình. Khởi tạo repositories Truy cập vào https://github.com/topics/jekyll-plugin và chọn một template bạn yêu thích từ jekelyy. Trong bài này tôi chọn template https://github.com/cotes2020/chirpy-starter. Fork repositories và đặt tên lại .github.io *Fork repositories từ chirpy-starter. *Đặt tên lại repositpries. *Kết quả. Cài đặtUbuntu Cài đặt Ruby và các điều kiện tiên quyết khác : sudo apt-get install ruby-full build-essential zlib1g-dev echo '# Install Ruby Gems to ~/gems' &gt;&gt; ~/.bashrcecho 'export GEM_HOME=\"$HOME/gems\"' &gt;&gt; ~/.bashrcecho 'export PATH=\"$HOME/gems/bin:$PATH\"' &gt;&gt; ~/.bashrcsource ~/.bashrc gem install jekyll bundler Config code Clone source code về máy để config git clone https://github.com/tlqbao/tlqbao.github.io.git cd tlqbao.github.io.git Cài đặt các gói phụ thuộc bundler Run local host bundle exec jekyll s Bạn sẽ tìm thấy hướng dẫn cụ thể cho việc config template này ở https://github.com/cotes2020/jekyll-theme-chirpy#documentation Trong bài viết này sẽ chỉ đề cập đến việc config những phần cơ bản, bạn có thể xem và config sâu hơn tùy ý theo tài liệu.Tất cả các phần tùy chỉnh đều nằm ở file _config.yml Title (line 20). title: ví dụ: title: tlqbao url (line 32) url: ‘' ví dụ: url: 'https://tlqbao.github.io' timezone (line 19) timezone: Asia/Ho_Chi_Minh tagline (line 26) tagline : &lt;câu nói yêu thích của bạn hây bất cứ thứ gì đó …&gt; Deploy blog Tạo commit và push code git add .git statusgit commit -m \"first config\"git push origin main Config git. Chọn Settings —&gt; Pages. Ở Deploy from branch chọn Github Action. Ở Deploy from branch chọn gh-pages. Kết quả: Thêm miền tùy chỉnhCó SSL Domain porkbunKhông SSL Domain venomTạo bài viết đầu tiênGiới thiệu ngôn ngữ MaskdownMột số syntax cơ bản trong Markdown.Markdown\tHTML\tKết quả| Markdown | HTML | Kết quả ||:——-|:——:|——-:|| # Header 1 | &lt;h1&gt;Header 1&lt;/h1&gt; | Header 1 || ## Header 2 | &lt;h2&gt;Header 2&lt;/h2&gt; | Header 2 || ### Header 3 | &lt;h3&gt;Header 3&lt;/h3&gt; | Header 3 || #### Header 4 | &lt;h4&gt;Header 4&lt;/h4&gt; | Header 4 || ====== | ====== | =====: || Footer | Footer | Footer |Các bước tạo bài viếtCác lỗi thường gặp và cách fix Lỗi abc Lỗi xyzTương lai Bài viết sẽ liên được cập nhật thường xuyên." }, { "title": "Phân loại âm thanh (P1)", "url": "/posts/Ph%C3%A2n-lo%E1%BA%A1i-%C3%A2m-thanh-(P1)/", "categories": "Python, Machine-learning", "tags": "ML, AI", "date": "2022-09-20 00:00:00 +0700", "snippet": "Cài đặt thư việnThư viện hỗ trợ phân tích âm thanh và âm nhạc là Librosa!pip install librosa!pip install tensorflowPhân tích dữ liệuimport IPython.display as ipdfilepath = \"archive/fold1/101415-3-0-2.wav\"ipd.Audio(filepath)Vì vậy, khi tải bất kỳ tệp âm thanh nào bằng Librosa, nó mang lại 2 điều. Một là tốc độ lấy mẫu và một là mảng hai chiều Tốc độ lấy mẫu - Nó thể hiện số lượng mẫu được ghi lại mỗi giây. Tốc độ lấy mẫu mặc định mà librosa đọc tệp là 22050 Mảng 2-D - Trục đầu tiên đại diện cho các biên độ mẫu đã ghi. Và trục thứ hai đại diện cho số lượng kênh. Có nhiều loại kênh khác nhau - Đơn âm (âm thanh có một kênh) và âm thanh nổi (âm thanh có hai kênh). !pip install matplotlibimport librosaimport librosa.displayimport matplotlib.pyplot as pltdata, sample_rate = librosa.load(filepath)plt.figure(figsize=(12, 5))librosa.display.waveshow(data, sr=sample_rate)Librosa hiện đang trở nên phổ biến để xử lý tín hiệu âm thanh vì ba lý do sau. Nó cố gắng hội tụ tín hiệu thành mono (một kênh). Nó có thể đại diện cho tín hiệu âm thanh từ -1 đến +1 (ở dạng chuẩn hóa), do đó, một mẫu thông thường được quan sát. Nó cũng có thể xem tốc độ mẫu và theo mặc định, nó chuyển đổi nó thành 22 kHz. Kiểm tra tập dữ liệu mất cân bằng!pip install pandasimport pandas as pdmetadata = pd.read_csv('csv/UrbanSound8K.csv')metadata.head(10)sử dụng hàm đếm giá trị để kiểm tra bản ghi của mỗi lớp.metadata['class'].value_counts()!pip install seabornimport seaborn as snsplt.figure(figsize=(10, 6))sns.countplot(metadata['class'])plt.title(\"Count of records in each class\")plt.xticks(rotation=\"vertical\")plt.show()Xử lý trước dữ liệuNhiệm vụ là trích xuất một số thông tin quan trọng và giữ dữ liệu ở dạng độc lập (Các tính năng được trích xuất từ ​​tín hiệu âm thanh) và các tính năng phụ thuộc (nhãn lớp).Sử dụng Mel Frequency Cepstral coefficients để trích xuất các tính năng độc lập từ tín hiệu âm thanhMFCC tóm tắt sự phân bố tần số trên kích thước cửa sổ. Vì vậy, có thể phân tích cả đặc tính tần số và thời gian của âm thanh. Biểu diễn âm thanh này sẽ cho phép xác định các đặc điểm để phân loại.mfccs = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=40)print(mfccs.shape)print(mfccs)mfccs.shape: https://stackoverflow.com/questions/65206575/what-are-the-components-of-the-mel-mfcc### Trích xuất các tính năng từ tất cả các tệp âm thanh và chuẩn bị khung dữ liệu!pip install numpyres_type: strTheo mặc định, điều này sử dụng chế độ chất lượng cao của resampy (‘kaiser_best’).Để sử dụng một phương pháp nhanh hơn, hãy đặt res_type = ‘kaiser_fast’.Để sử dụng scipy.signal.resample, hãy đặt res_type = ‘scipy’.import numpy as npdef features_extractor(file_name): audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40) # để tìm hiểu các tính năng được chia tỷ lệ, chúng ta sẽ tìm giá trị trung bình của sự chuyển vị của một mảng mfccs_scaled_features = np.mean(mfccs_features.T,axis=0) return mfccs_scaled_featuresthư viện python TQDM để theo dõi tiến trình!pip install tqdmimport numpy as npfrom tqdm import tqdmimport osextracted_features=[]for index_num,row in tqdm(metadata.iterrows()): file_name = os.path.join('archive/fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"])) final_class_labels=row[\"class\"] data=features_extractor(file_name) extracted_features.append([data,final_class_labels])extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])extracted_features_df.head()Train Test splitTách tập dữ liệu thành tập dữ liệu độc lập và phụ thuộcX=np.array(extracted_features_df['feature'].tolist())y=np.array(extracted_features_df['class'].tolist())Mã hóa nhãn thành số nguyênfrom tensorflow.keras.utils import to_categoricalfrom sklearn.preprocessing import LabelEncoderlabelencoder=LabelEncoder()y=to_categorical(labelencoder.fit_transform(y))chia dữ liệu thành các tập huấn luyện và thử nghiệm theo tỷ lệ 80-20from sklearn.model_selection import train_test_splitX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)Tạo mô hình phân loại âm thanhANN với 3 lớp dày đặc và kiến ​​trúc: Lớp đầu tiên có 100 tế bào thần kinh. Hình dạng đầu vào là 40 theo số lượng tính năng có chức năng kích hoạt là Relu và để tránh bất kỳ trang bị quá mức nào, chúng tôi sẽ sử dụng lớp Dropout với tỷ lệ 0,5. Lớp thứ hai có 200 tế bào thần kinh có chức năng kích hoạt là Relu và lớp Dropout có tỉ lệ là 0,5. Lớp thứ ba lại có 100 tế bào thần kinh với kích hoạt là Relu và lớp Dropout có tỷ lệ là 0,5. from tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Dense,Dropout,Activation,Flattenfrom tensorflow.keras.optimizers import Adamfrom sklearn import metricsnum_labels=y.shape[1]model=Sequential()###first layermodel.add(Dense(100,input_shape=(40,)))model.add(Activation('relu'))model.add(Dropout(0.5))###second layermodel.add(Dense(200))model.add(Activation('relu'))model.add(Dropout(0.5))###third layermodel.add(Dense(100))model.add(Activation('relu'))model.add(Dropout(0.5))###final layermodel.add(Dense(num_labels))model.add(Activation('softmax'))model.summary()Biên dịch mô hìnhĐể biên dịch mô hình, chúng ta cần xác định loss function là cross-entropy, accuracy metrics là accuracy score và optimizer là Adam.model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')Train modelĐào tạo mô hình và lưu mô hình ở định dạng HDF5.Sử dụng callback, đây là một điểm kiểm tra để biết cần bao nhiêu thời gian để đào tạo qua dữ liệu.Bằng cách đặt verbose = 0, 1 hoặc 2, để ‘xem’ tiến trình đào tạo cho mỗi kỷ nguyên như thế nào.verbose=0 sẽ không cho bạn thấy gì (im lặng)verbose=1 sẽ hiển thị cho bạn một thanh tiến trình progres_barverbose=2 sẽ chỉ đề cập đến số kỷ nguyên: Epoch 1/10 …from tensorflow.keras.callbacks import ModelCheckpointfrom datetime import datetime num_epochs = 100num_batch_size = 32checkpointer = ModelCheckpoint(filepath='./audio_classification.hdf5', verbose=1, save_best_only=True)(xuất ra các trọng số của mô hình mỗi khi quan sát thấy sự cải thiện trong quá trình đào tạo.)# checkpoint# filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"# checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')# callbacks_list = [checkpoint] (Sử dụng EarlyStopping cùng với Checkpoint)# checkpoint# from tensorflow.keras.callbacks import EarlyStopping# filepath=\"weights.best.hdf5\"# checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')# es = EarlyStopping(monitor='val_accuracy', patience=5)# -&gt;Quá trình huấn luyện này đã dừng lại nếu không có độ chính xác nào đạt được tốt hơn trong năm kỷ nguyên vừa qua# callbacks_list = [checkpoint, es]start = datetime.now()model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)duration = datetime.now() - startprint(\"Training completed in time: \", duration)Kiểm tra độ chính xác của Testtest_accuracy=model.evaluate(X_test,y_test,verbose=0)print(test_accuracy[1])sử dụng thuộc tính metrics_names của mô hình để tìm hiểu xem mỗi giá trị trả về của model.evaluate tương ứng với cái gìmodel.metrics_namesDự đoán lớp tương ứng cho mỗi tệp âm thanhpredict_x=model.predict(X_test) classes_x=np.argmax(predict_x,axis=1)print(classes_x)Kiểm tra một số mẫu âm thanh thử nghiệmfilename=\"archive/fold7/101848-9-0-0.wav\"#preprocess the audio fileaudio, sample_rate = librosa.load(filename, res_type='kaiser_fast') mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)#Reshape MFCC feature to 2-D arraymfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)#predicted_label=model.predict_classes(mfccs_scaled_features)x_predict=model.predict(mfccs_scaled_features) predicted_label=np.argmax(x_predict,axis=1)print(predicted_label)prediction_class = labelencoder.inverse_transform(predicted_label) print(prediction_class)" }, { "title": "Python Kivy | Tạo ứng dụng điện thoại thông minh với Kivy và Buildozer", "url": "/posts/Python-Kivy-T%E1%BA%A1o-%E1%BB%A9ng-d%E1%BB%A5ng-%C4%91i%E1%BB%87n-tho%E1%BA%A1i-th%C3%B4ng-minh-v%E1%BB%9Bi-Kivy-v%C3%A0-Buildozer/", "categories": "Python, Kivy", "tags": "kivy, mobile app, buildozer", "date": "2022-09-18 00:00:00 +0700", "snippet": "Trong bài viết này, chúng ta cùng tìm hiểu về framework Kivy, cách xây dựng một ứng dụng điện thoại với Kivy và build thành app android với buildozer.Sơ lượcGiới thiệu framework djangoMục tiêu bài viếtCác liên kết và nguồn tài liệu tham khảoCài đặtKhởi tạo môi trườngCài đặt các thư viện yêu cầuỨng dụng đầu tiênỨng dụng nâng caoKhởi tạo môi trường build trên Ubuntu 20.04Các thư viện yêu cầuThư viện cần thiết sudo apt-get install python3-distutils sudo python3 get-pip.py sudo apt-get install -y python3-pip build-essential git python3 python3-dev sudo apt-get install -y libsdl2-dev libsdl2-image-dev libsdl2-mixer-dev libsdl2-ttf-dev libportmidi-dev libswscale-dev libavformat-dev libavcodec-dev zlib1g-dev sudo apt-get install cython sudo pip3 install kivy python3 main.py - code is available sudo apt-get install libltdl-dev libffi-dev libssl-dev autoconf autotools-dev sudo apt install -y git zip unzip openjdk-8-jdk python3-pip autoconf libtool pkg-config zlib1g-dev libncurses5-dev libncursesw5-dev libtinfo5 cmake libffi-dev libssl-dev pip3 install --user --upgrade Cython==0.29.19 virtualenv # the --user should be removed if you do this in a venv Thêm dòng sau vào cuối tệp ~ / .bashrc của bạn export PATH=$PATH:~/.local/bin/Clone source code buildozer git clone https://github.com/kivy/buildozer.git cd buildozer sudo python3 setup.py install buildozer init buildozer android debugBuild app adroidCác lỗi thường gặp và cách sửaTương lai*Bài viết sẽ được cập nhật thường xuyên." }, { "title": "Test post number 2", "url": "/posts/Test-post-number-2/", "categories": "Python, Machine-learning", "tags": "AI, ML", "date": "2022-09-17 00:00:00 +0700", "snippet": "Test thử bài viết số 2" }, { "title": "Python hat - Evil Twin Attack (P.2)", "url": "/posts/Python-hat-Evil-Twin-Attack-(P.2)/", "categories": "Python, Hat", "tags": "hat, wifi", "date": "2022-09-17 00:00:00 +0700", "snippet": "Tiếp nối bài viết Tấn công mạng với ngôn ngữ Python phần 1 (DDoS-wifi) ,hôm nay mình chia sẻ phần 2 về kỹ thuật tấn công Wifi bằng điểm truy cập giả mạo. Để tiện trong việc quan sát và thực hành,các bạn có thể xem video theo link bên dưới.Evil Twin Attack (P.2) - Video." }, { "title": "Python Django | Tạo webapp với framework Django", "url": "/posts/Python-Django-T%E1%BA%A1o-webapp-v%E1%BB%9Bi-framework-Django/", "categories": "Python, Django", "tags": "django, web", "date": "2022-09-17 00:00:00 +0700", "snippet": "Một bài viết đầy đủ từ tạo, config, nâng cấp và deploy web lên server trên các nền tảng khác nhau.Sơ lượcGiới thiệu framework django Django là khung ứng dụng web cho Python Django mới nhất hỗ trợ dòng python 3Mục tiêu bài viết Cung cấp kiến thức đầy đủ về việc thiết lập cơ bản cho đến tạo ra một web cơ bản với đầy đủ chức năng. Ghi lại kinh nghiệm hơn 1 năm làm việc và học tập trên nền tảng này.Các liên kết và nguồn tài liệu tham khảoTài liệu từ Django https://docs.djangoproject.com/en/4.0/Tài liệu từ geeksforgeeks https://www.geeksforgeeks.org/django-tutorial/Tài liệu từ tomomano.gitlab.io https://tomomano.gitlab.io/intro-aws/#aws_accountCài đặtKhởi tạo môi trường ảoViệc khởi tạo môi trường ảo để phục vụ cho project là cần thiết, tránh sự ảnh hưởng từ môi trường máy đến project và ngược lại. Trong bài viết này chỉ đề cập đến môi trường ảo trên ubuntu (trên các hệ điều hành khác sẽ không được đề cập tại đây) Cài đặt thư viện python3-venv sudo apt-get install -y python3-venv Tạo folder dự án mkdir my_project Di chuyển cào folder dự án cd my_project Tạo môi trường ảo python -m venv django-env Kích hoặt môi trường ảo source django-env/bin/activate Hủy kích hoạt môi trường deactivate Cài đặt các thư viện yêu cầuĐể thuận tiện cho quá trình thêm và cài đặt các thư viện yêu cầu cần thiết trong dự trên venv chúng ta nên tạo file requirements.txt. Ở đây chúng ta không sử dụng “pip freeze” tránh trường hợp cài quá nhiều thư viện không cần thiết. Tạo file requirements.txt touch requirements.txt Cài đặt các thư viện yêu cầu trong file requirements.txt pip install -r requirements.txt Tạo ứng dụng web django đầu tiên Tạo project django-admin startproject web_project Tạo web app bên trong project Di chuyển vào folder dự án cd web_project Tạo web app python manage.py startapp my_web Chạy thử nghiệm python manage.py runserver Tìm hiểu về mô hình MVT (Models - View - Templates)Tùy chỉnh cơ bảnÁp dụngTạo blog cá nhânTạo webapp djangoThiết kế Frontend với NicepageKết hợp django với templates từ nicepageDeloyAWSHerokuNâng cấp blogThêm gói ckeditor vào blogThêm slug và tagit vào djangoThêm cloudinary lưu trữ bên ngoài herokuThêm miền tùy chỉnhTạo tên miền .tk miễn phíThiết lập tên miền miễn phí và herokuNhững lỗi thường gặp và cách sửaTương lai Bài viết sẽ luôn được cập nhật thường xuyên." } ]
